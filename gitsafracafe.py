# -*- coding: utf-8 -*-
"""gitsafracafe

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wVYLBPAs7MTVZrGCmEXStXqU1QdkPSzJ
"""

!pip install geemap geopandas ipyleaflet ipywidgets
!pip install shapely
!pip install -U geemap
!pip install SpeechRecognition
!pip install geocoder
!pip install geemap geopandas ipyleaflet ipywidgets shapely speechrecognition geocoder
!pip install fiona geopandas geemap ipywidgets speechrecognition
!pip install libgdal-dev
!pip install fiona --no-binary fiona

# Imports essenciais
import os
import json
import time
import random
import string
import numpy as np
import geemap
import geocoder
import ee
import geopandas as gpd
from shapely.geometry import Point, mapping, shape as shapely_shape
from IPython.display import display
from ipywidgets import FileUpload
from ipywidgets import (
    Button, Dropdown, Output, VBox, HBox, HTML, FloatText,
    ToggleButton, Layout, Checkbox
)
import speech_recognition as sr
from IPython.display import display, clear_output
import zipfile
import pandas as pd
from geemap import geojson_to_ee
import speech_recognition as sr
from ipyfilechooser import FileChooser
from shapely.geometry import shape
try:
    import fiona
    from fiona import drivers
    HAS_FIONA = True
except ImportError:
    HAS_FIONA = False
    print("Instalando biblioteca Fiona...")
    !pip install fiona
    try:
        import fiona
        from fiona import drivers
        HAS_FIONA = True
    except ImportError:
        print("N√£o foi poss√≠vel instalar a biblioteca Fiona. Algumas funcionalidades ser√£o limitadas.")

# Autentica√ß√£o e inicializa√ß√£o do Earth Engine
try:
    ee.Initialize(project='agriculturadeprecisao')
except:
    ee.Authenticate()
    ee.Initialize(project='agriculturadeprecisao')

# Inicializa o mapa
Map = geemap.Map(center=[-15, -55], zoom=4)
Map.add_basemap('HYBRID')
Map.layout.height = '600px'

# Vari√°veis globais
gdf_poligono = None
gdf_pontos = None
gdf_poligono_total = None
unidade_selecionada = 'kg'
ponto_atual = None
dialogo_produtividade = None
dialogo_voz = None
modo_desenho = None
densidade_plantas = None
produtividade_media = None

# Output principal
output = Output()

# Widgets de controle de produtividade
unidade_dropdown = Dropdown(
    options=['kg', 'latas', 'litros'],
    value='kg',
    description='Unidade:'
)
uploader = FileUpload(
    accept='.gpkg,.shp,.shx,.dbf,.kml,.kmz',
    multiple=True,
    description='Selecionar arquivo'
)

# -----------------------------
# WIDGET PARA DENSIDADE DE PLANTAS
# -----------------------------
densidade_input = FloatText(
    description='Plantas por hectare:',
    value=0.0,
    style={'description_width': 'initial'}
)
btn_inserir_densidade = Button(description='Inserir valor', button_style='info')

produtividade_input = FloatText(
    description='Produtividade m√©dia (sacas/ha):',
    value=0.0,
    style={'description_width': 'initial'}
)
btn_inserir_produtividade = Button(description='Inserir valor', button_style='info')

def handle_densidade(b):
    global densidade_plantas
    densidade_plantas = densidade_input.value
    with output:
        output.clear_output()
        print(f"‚úÖ Densidade definida: {densidade_plantas} plantas/hectare")

def handle_produtividade(b):
    global produtividade_media
    produtividade_media = produtividade_input.value
    with output:
        output.clear_output()
        print(f"‚úÖ Produtividade m√©dia definida: {produtividade_media} sacas/hectare")

btn_inserir_densidade.on_click(handle_densidade)
btn_inserir_produtividade.on_click(handle_produtividade)

# Bot√µes principais
btn_adicionar_poligono = Button(description='‚ñ∂Ô∏è √Årea amostral', button_style='primary')
btn_adicionar_poligono_2 = Button(description='‚ñ∂Ô∏è √Årea total', button_style='primary')
btn_limpar_poligono = Button(description='üóëÔ∏è Limpar √Årea', button_style='danger', disabled=True)
btn_gerar_pontos_poligono = Button(description='üî¢ Gerar pontos', button_style='info', disabled=True)
btn_inserir_manual = Button(description='‚úèÔ∏è Inserir pontos', button_style='info', disabled=True)
btn_inserir_voz = Button(description='üé§ Inserir por Voz', button_style='warning', disabled=True)
btn_inserir_prod = Button(description='üìù Inserir produtividade', button_style='success', disabled=True)
btn_salvar_pontos = Button(description='üíæ Salvar pontos', button_style='success', disabled=True)
exportar_btn = Button(description='üíæ Exportar dados', button_style='info', icon='save')

# Bot√µes adicionais
btn_adicionar_pontos = Button(description='üîò Adicionar pontos', button_style='info', disabled=True)
btn_gerar_auto = Button(description='ü§ñ Gerar automaticamente', button_style='primary')
btn_inserir_voz_gps = Button(description='üé§ Inserir por voz (GPS)', button_style='warning')
btn_inserir_manual_gps = Button(description='‚úèÔ∏è Inserir pontos (GPS)', button_style='warning')
btn_ativar_gps = ToggleButton(description='üìç Ativar GPS', value=False, button_style='success', icon='satellite')
btn_carregar_arquivo = Button(description='üìÇ Carregar arquivo', button_style='info')

# Widget para sele√ß√£o de arquivo
file_chooser = FileChooser()
file_chooser.default_path = os.getcwd()
file_chooser.filter_pattern = '*.shp','.kml','.kmz','.gpkg'
file_chooser.title = 'Selecione pol√≠gono'
file_chooser.layout.display = 'none'

# Painel com op√ß√µes adicionais para coleta
pontos_options = VBox([
    HTML("<h4>M√©todo de Adi√ß√£o de Pontos:</h4>"),
    btn_gerar_auto,
    btn_inserir_voz_gps,
    btn_inserir_manual_gps,
    btn_ativar_gps
])
pontos_options.layout.display = 'none'

# Layout de controle principal
controls_column = VBox([
    file_chooser,
    HTML("<h3>Carregar arquivo:</h3>"),
    uploader,
    btn_adicionar_poligono,
    btn_adicionar_poligono_2,
    btn_gerar_pontos_poligono,
    btn_limpar_poligono,
    btn_adicionar_pontos,
    pontos_options,
    HTML("<h4>Produtividade</h4>"),
    unidade_dropdown,
    btn_inserir_prod,
    btn_inserir_voz,
    btn_inserir_manual,
    btn_salvar_pontos,
    exportar_btn
])

# Adicionar ao controls_column ap√≥s o uploader
controls_column.children = (
    [controls_column.children[0]] +
    [controls_column.children[1]] +
    [controls_column.children[2]] +
    [HTML("<h4>Par√¢metros da √Årea</h4>"),
     densidade_input,
     btn_inserir_densidade,
     produtividade_input,
     btn_inserir_produtividade] +
    list(controls_column.children[3:])
)

app_layout = HBox([controls_column, Map])

# -----------------------------
# FUN√á√ïES AUXILIARES PRINCIPAIS
# -----------------------------
def gerar_codigo():
    letras = ''.join(random.choices(string.ascii_uppercase + string.digits, k=4))
    numeros = ''.join(random.choices(string.digits, k=2))
    return f"{letras}-{numeros}-{''.join(random.choices(string.ascii_uppercase + string.digits, k=4))}"

def converter_para_kg(valor, unidade):
    """Converte valores para kg conforme especificado"""
    if pd.isna(valor):
        return 0
    try:
        valor = float(valor)
    except:
        return 0

    if unidade == 'kg':
        return valor
    elif unidade == 'latas':
        return valor * 1.8
    elif unidade == 'litros':
        return valor * 0.09
    else:
        return valor

def get_utm_epsg(lon, lat):
    utm_zone = int((lon + 180) / 6) + 1
    return 32600 + utm_zone if lat >= 0 else 32700 + utm_zone

# -----------------------------
# FUN√á√ïES DE INTERA√á√ÉO COM O MAPA
# -----------------------------
def toggle_pontos_options(b):
    pontos_options.layout.display = 'block' if pontos_options.layout.display == 'none' else 'none'
    btn_adicionar_pontos.button_style = 'success' if pontos_options.layout.display == 'block' else 'info'

    with output:
        output.clear_output()
        if pontos_options.layout.display == 'block':
            print("üîò Modo de adi√ß√£o de pontos ativado")
            print("Clique no mapa para adicionar pontos manualmente")

# Limpar pol√≠gono
def limpar_poligono(b):
    global gdf_poligono, gdf_pontos
    for layer in Map.layers:
        if layer.name == "√Årea de Interesse" or layer.name.startswith("Ponto "):
            Map.remove_layer(layer)
    gdf_poligono = None
    gdf_pontos = None
    for btn in [btn_gerar_pontos_poligono, btn_limpar_poligono, btn_salvar_pontos,
                btn_inserir_prod, btn_inserir_voz, btn_inserir_manual, btn_adicionar_pontos]:
        btn.disabled = True
    pontos_options.layout.display = 'none'
    file_chooser.layout.display = 'none'
    btn_adicionar_pontos.button_style = 'info'
    with output:
        output.clear_output()
        print("√Årea removida. Desenhe novamente ou carregue um arquivo.")

btn_limpar_poligono.on_click(limpar_poligono)

# Desenhar pol√≠gono da √°rea amostral
def handle_draw(target, action, geo_json):
    global gdf_poligono
    if geo_json['geometry']['type'] == 'Polygon':
        for layer in Map.layers:
            if layer.name == "√Årea amostral":
                Map.remove_layer(layer)
        try:
            gdf_poligono = gpd.GeoDataFrame(geometry=[shapely_shape(geo_json['geometry'])], crs='EPSG:4326')
            feature = ee.Feature(ee.Geometry(geo_json['geometry']), {'name': 'area_interesse'})
            Map.addLayer(feature, {'color': 'red', 'fillColor': 'red', 'fillOpacity': 0.2}, "√Årea de Interesse")
            btn_gerar_pontos.disabled = False
            btn_limpar_poligono.disabled = False
            btn_adicionar_pontos.disabled = False
            with output:
                output.clear_output()
                print("‚úÖ √Årea de interesse definida!")
        except Exception as e:
            with output:
                print(f"Erro: {e}")

# Criar ponto amostral
def iniciar_desenho_amostral(b):
    global modo_desenho
    modo_desenho = 'amostral'
    Map.draw_control.clear()

btn_adicionar_poligono.on_click(iniciar_desenho_amostral)

# criar √°rea total
def handle_draw_total(target, action, geo_json):
    global gdf_poligono_total
    if geo_json['geometry']['type'] == 'Polygon':
        try:
            gdf_poligono_total = gpd.GeoDataFrame(geometry=[shapely_shape(geo_json['geometry'])], crs='EPSG:4326')
            feature = ee.Feature(ee.Geometry(geo_json['geometry']), {'name': 'area_total'})
            Map.addLayer(feature, {'color': 'blue', 'fillColor': 'blue', 'fillOpacity': 0.15}, "√Årea Total")
            with output:
                output.clear_output()
                print("‚úÖ √Årea total definida!")
        except Exception as e:
            with output:
                print(f"Erro: {e}")

def iniciar_desenho_total(b):
    global modo_desenho
    modo_desenho = 'total'
    Map.draw_control.clear()

btn_adicionar_poligono_2.on_click(iniciar_desenho_total)

def verificar_areas_criadas():
    """Verifica se ambas as √°reas (amostral e total) foram criadas e habilita os bot√µes de gera√ß√£o de pontos"""
    if gdf_poligono is not None and gdf_poligono_total is not None:
        btn_gerar_pontos_poligono.disabled = False  # Habilita bot√£o para √°rea amostral
        with output:
            print("‚úÖ √Åreas amostral e total definidas!")
            print("Gere pontos da √°rea amostral")

# verifica√ß√£o
def handle_draw_unificado(target, action, geo_json):
    global modo_desenho, gdf_poligono, gdf_poligono_total
    geometria = shape(geo_json["geometry"])
    gdf_temp = gpd.GeoDataFrame(geometry=[geometria], crs="EPSG:4326")

    if modo_desenho == 'amostral':
        gdf_poligono = gdf_temp
        with output:
            clear_output()
            print("‚úÖ √Årea amostral definida.")
        Map.add_gdf(gdf_poligono, layer_name="√Årea Amostral", style={"color": "blue"})
    elif modo_desenho == 'total':
        gdf_poligono_total = gdf_temp
        with output:
            clear_output()
            print("‚úÖ √Årea total definida.")
        Map.add_gdf(gdf_poligono_total, layer_name="√Årea Total", style={"color": "green"})

    # Verifica se ambas as √°reas foram criadas
    verificar_areas_criadas()

# Conecte a fun√ß√£o ao mapa
Map.draw_control.on_draw(handle_draw_unificado)

# -----------------------------
# FUN√á√ïES DE CARREGAMENTO DE ARQUIVOS
# -----------------------------

def carregar_arquivo(b):
    """Ativa/desativa seletor de arquivos"""
    try:
        file_chooser.reset()
        file_chooser.layout.display = 'block' if file_chooser.layout.display == 'none' else 'none'
        with output:
            if file_chooser.layout.display == 'block':
                print("üîç Selecione o arquivo")
            else:
                print("Seletor de arquivos fechado")
    except Exception as e:
        with output:
            print(f"‚ùå Erro ao abrir seletor de arquivos: {str(e)}")

btn_carregar_arquivo.on_click(carregar_arquivo)

def ler_arquivo_geografico(file_path):
    """L√™ arquivos geogr√°ficos nos formatos suportados"""
    if file_path.endswith('.gpkg'):
        return gpd.read_file(file_path)

    elif file_path.endswith('.shp'):
        base_name = os.path.splitext(file_path)[0]
        required_files = [f'{base_name}.{ext}' for ext in ['shp', 'shx', 'dbf']]
        missing_files = [f for f in required_files if not os.path.exists(f)]
        if missing_files:
            raise FileNotFoundError(f"Arquivos do shapefile incompletos: {', '.join(missing_files)}")
        return gpd.read_file(file_path)

    elif file_path.endswith('.kml'):
        return gpd.read_file(file_path, driver='KML')

    elif file_path.endswith('.kmz'):
        with zipfile.ZipFile(file_path, 'r') as kmz:
            kml_files = [f for f in kmz.namelist() if f.endswith('.kml')]
            if not kml_files:
                raise FileNotFoundError("Nenhum arquivo KML encontrado dentro do KMZ")
            with kmz.open(kml_files[0]) as kml:
                return gpd.read_file(kml, driver='KML')

    else:
        raise ValueError("‚ùå Formato de arquivo n√£o suportado! Formatos aceitos: .gpkg, .shp, .kml, .kmz")

def processar_poligono_no_mapa(gdf, nome_layer="√Årea de Interesse"):
    """Valida e plota pol√≠gono no mapa"""
    global gdf_poligono

    if not any(gdf.geometry.type.isin(['Polygon', 'MultiPolygon'])):
        raise ValueError("‚ö†Ô∏è O arquivo n√£o cont√©m pol√≠gonos!")

    gdf_poligono = gdf.to_crs("EPSG:4326")

    for layer in Map.layers:
        if layer.name == nome_layer:
            Map.remove_layer(layer)

    feature = geemap.geojson_to_ee(gdf_poligono.__geo_interface__)
    Map.addLayer(feature, {'color': 'red', 'fillColor': 'red', 'fillOpacity': 0.2}, nome_layer)

    btn_gerar_pontos_poligono.disabled = False
    btn_limpar_poligono.disabled = False
    btn_adicionar_pontos.disabled = False

# -----------------------------
# CALLBACK: FILECHOOSER
# -----------------------------
def handle_file_selection(change):
    """Processa arquivos selecionados via FileChooser"""
    if not change['new']:
        return

    file_path = change['new']

    try:
        with output:
            output.clear_output()
            print(f"‚è≥ Processando arquivo: {file_path}")

        # Verifica se arquivo existe
        if not os.path.exists(file_path):
            alt_path = f'/content/{os.path.basename(file_path)}'
            if os.path.exists(alt_path):
                file_path = alt_path
            else:
                raise FileNotFoundError(f"Arquivo n√£o encontrado: {file_path}")

        gdf = ler_arquivo_geografico(file_path)
        processar_poligono_no_mapa(gdf)

        with output:
            print(f"‚úÖ Arquivo carregado com sucesso!")
            print(f"Tipo: {gdf_poligono.geometry.type[0]}")
            print(f"Pol√≠gonos: {len(gdf_poligono)}")

    except Exception as e:
        with output:
            print(f"‚ùå Erro ao processar arquivo: {str(e)}")

# -----------------------------
# CALLBACK: FILEUPLOAD
# -----------------------------
def handle_upload(change):
    """Processa arquivos enviados via FileUpload"""
    if not change['new']:
        return

    upload = change['owner']
    try:
        if not upload.value:
            return

        filename, fileinfo = next(iter(upload.value.items()))
        content = fileinfo['content']
        temp_path = f'/content/{filename}'

        with open(temp_path, 'wb') as f:
            f.write(content)

        with output:
            output.clear_output()
            print(f"‚è≥ Processando arquivo enviado: {filename}")

        gdf = ler_arquivo_geografico(temp_path)
        processar_poligono_no_mapa(gdf)

        with output:
            print(f"‚úÖ Arquivo {filename} carregado com sucesso!")
            print(f"Tipo: {gdf_poligono.geometry.type[0]}")
            print(f"Pol√≠gonos: {len(gdf_poligono)}")

        os.remove(temp_path)

    except Exception as e:
        with output:
            print(f"‚ùå Erro ao processar arquivo: {str(e)}")

# -----------------------------
# INTERFACE WIDGETS
# -----------------------------
file_chooser = FileChooser(
    path=os.getcwd(),
    filename='',
    filter_pattern='*.gpkg;*.shp;*.kml;*.kmz',
    title='Selecione o arquivo com o pol√≠gono'
)
file_chooser.layout.display = 'none'
file_chooser.register_callback(handle_file_selection)

uploader = FileUpload(
    accept='.gpkg,.shp,.shx,.dbf,.kml,.kmz',
    multiple=False,
    description='Selecionar arquivo'
)
uploader.observe(handle_upload, names='value')

# -----------------------------
# FUN√á√ïES DE GERA√á√ÉO DE PONTOS
# -----------------------------
def gerar_pontos_automaticos(b):
    global gdf_pontos
    if gdf_poligono is None:
        with output:
            print("‚ö†Ô∏è Defina a √°rea primeiro!")
        return

    centroid = gdf_poligono.geometry.centroid.iloc[0]
    epsg = get_utm_epsg(centroid.x, centroid.y)
    gdf_utm = gdf_poligono.to_crs(epsg=epsg)
    area_ha = gdf_utm.geometry.area.sum() / 10000
    lado = np.sqrt(5000)  # 2 pontos por hectare (5000 m¬≤ por ponto)

    bounds = gdf_utm.total_bounds
    x_coords = np.arange(bounds[0], bounds[2], lado)
    y_coords = np.arange(bounds[1], bounds[3], lado)

    pontos = [Point(x, y) for x in x_coords for y in y_coords if gdf_utm.geometry.iloc[0].contains(Point(x, y))]
    gdf_pontos = gpd.GeoDataFrame(geometry=pontos, crs=gdf_utm.crs).to_crs("EPSG:4326")

    # Adiciona todas as colunas necess√°rias
    gdf_pontos['Code'] = [gerar_codigo() for _ in range(len(gdf_pontos))]
    gdf_pontos['valor'] = 0
    gdf_pontos['unidade'] = 'kg'
    gdf_pontos['maduro_kg'] = 0
    gdf_pontos['coletado'] = False
    gdf_pontos['latitude'] = gdf_pontos.geometry.y
    gdf_pontos['longitude'] = gdf_pontos.geometry.x
    gdf_pontos['metodo'] = 'auto'

    # Adiciona pontos ao mapa
    for idx, ponto in gdf_pontos.iterrows():
        marker = geemap.geojson_to_ee({
            "type": "Feature",
            "geometry": mapping(ponto.geometry),
            "properties": {"status": "N√£o coletado"}
        })
        Map.add_layer(marker, {'color': 'orange'}, f"Ponto {idx+1}")

    with output:
        output.clear_output()
        print(f"‚úÖ {len(gdf_pontos)} pontos gerados automaticamente!")
        print(f"√Årea total: {area_ha:.2f} hectares")

    # Ativa bot√µes
    for btn in [btn_salvar_pontos, btn_inserir_prod, btn_inserir_voz, btn_inserir_manual]:
        btn.disabled = False

btn_gerar_pontos_poligono.on_click(gerar_pontos_automaticos)

# -----------------------------
# FUN√á√ïES DE ADI√á√ÉO MANUAL DE PONTOS
# -----------------------------
def adicionar_ponto_manual_gps(b):
    if not btn_ativar_gps.value:
        with output:
            print("‚ö†Ô∏è Ative o GPS primeiro!")
        return

    # Simula√ß√£o de coleta de GPS - na pr√°tica integraria com biblioteca de geolocaliza√ß√£o
    lat, lon = -15.123, -55.456  # Exemplo - substituir por coordenadas reais
    adicionar_ponto(lat, lon, "manual_gps")

def adicionar_ponto_manual(b):
    # Cria interface para inser√ß√£o manual de coordenadas
    lat_input = FloatText(description='Latitude:', value=-15.0)
    lon_input = FloatText(description='Longitude:', value=-55.0)
    confirm_btn = Button(description='Confirmar', button_style='success')

    def confirmar_ponto(btn):
        lat = lat_input.value
        lon = lon_input.value
        adicionar_ponto(lat, lon, "manual")
        output.clear_output()

    confirm_btn.on_click(confirmar_ponto)

    with output:
        output.clear_output()
        display(VBox([lat_input, lon_input, confirm_btn]))

def adicionar_ponto(lat, lon, metodo):
    global gdf_pontos

    ponto = Point(lon, lat)

    if gdf_pontos is None:
        gdf_pontos = gpd.GeoDataFrame(columns=[
            'geometry', 'Code', 'valor', 'unidade', 'maduro_kg',
            'coletado', 'latitude', 'longitude', 'metodo'
        ], geometry='geometry', crs="EPSG:4326")

    novo_ponto = {
        'geometry': ponto,
        'Code': gerar_codigo(),
        'valor': 0,
        'unidade': unidade_dropdown.value,
        'maduro_kg': 0,
        'coletado': False,
        'latitude': lat,
        'longitude': lon,
        'metodo': metodo
    }

    gdf_pontos = gpd.GeoDataFrame(pd.concat([gdf_pontos, pd.DataFrame([novo_ponto])]), crs="EPSG:4326")

    # Adiciona ao mapa
    marker = geemap.geojson_to_ee({
        "type": "Feature",
        "geometry": mapping(ponto),
        "properties": {"status": "N√£o coletado"}
    })
    Map.add_layer(marker, {'color': 'blue'}, f"Ponto {len(gdf_pontos)}")

    # Ativa bot√µes
    btn_inserir_prod.disabled = False
    btn_inserir_voz.disabled = False
    btn_inserir_manual.disabled = False
    btn_salvar_pontos.disabled = False

    with output:
        print(f"‚úÖ Ponto {len(gdf_pontos)} adicionado ({metodo})")

btn_inserir_manual_gps.on_click(adicionar_ponto_manual_gps)
btn_inserir_manual.on_click(adicionar_ponto_manual)

# Adicione esta fun√ß√£o para criar pontos clicando no mapa
def handle_map_click(**kwargs):
    global ponto_atual, dialogo_produtividade

    if btn_adicionar_pontos.disabled or pontos_options.layout.display == 'none':
        return

    if kwargs.get('type') == 'click':
        # Obt√©m as coordenadas do clique
        coords = kwargs.get('coordinates')
        lon, lat = coords[0], coords[1]

        # Adiciona o ponto manualmente
        adicionar_ponto(lat, lon, "manual_mapa")

        # Prepara para inserir produtividade
        ponto_atual = len(gdf_pontos) - 1
        abrir_dialogo_produtividade(ponto_atual)

# Fun√ß√£o para abrir di√°logo de produtividade
def abrir_dialogo_produtividade(idx):
    global dialogo_produtividade

    # Cria os widgets do di√°logo
    valor_input = FloatText(description='Produtividade:', value=gdf_pontos.at[idx, 'valor'])
    unidade_input = Dropdown(
        options=['kg', 'latas', 'litros'],
        value=gdf_pontos.at[idx, 'unidade'],
        description='Unidade:'
    )
    salvar_btn = Button(description='Salvar', button_style='success')
    cancelar_btn = Button(description='Cancelar', button_style='warning')

    def salvar_dados(b):
        gdf_pontos.at[idx, 'valor'] = valor_input.value
        gdf_pontos.at[idx, 'unidade'] = unidade_input.value
        gdf_pontos.at[idx, 'maduro_kg'] = converter_para_kg(valor_input.value, unidade_input.value)
        gdf_pontos.at[idx, 'coletado'] = True

        # Atualiza o marcador no mapa
        for layer in Map.layers:
            if layer.name == f"Ponto {idx+1}":
                Map.remove_layer(layer)

        marker = geemap.geojson_to_ee({
            "type": "Feature",
            "geometry": mapping(gdf_pontos.at[idx, 'geometry']),
            "properties": {"status": "Coletado"}
        })
        Map.add_layer(marker, {'color': 'green'}, f"Ponto {idx+1}")

        with output:
            output.clear_output()
            print(f"‚úÖ Dados salvos no Ponto {idx+1}")

        dialogo_produtividade.close()

    def cancelar(b):
        dialogo_produtividade.close()
        with output:
            print("Opera√ß√£o cancelada")

    salvar_btn.on_click(salvar_dados)
    cancelar_btn.on_click(cancelar)

    # Cria o di√°logo
    dialogo_produtividade = VBox([
        HTML(f"<h4>Inserir Produtividade - Ponto {idx+1}</h4>"),
        HTML(f"Coordenadas: Lat {gdf_pontos.at[idx, 'latitude']:.5f}, Lon {gdf_pontos.at[idx, 'longitude']:.5f}"),
        valor_input,
        unidade_input,
        HBox([salvar_btn, cancelar_btn])
    ])

    # Exibe o di√°logo
    with output:
        output.clear_output()
        display(dialogo_produtividade)

# Atualize a fun√ß√£o adicionar_ponto para manter consist√™ncia
def adicionar_ponto(lat, lon, metodo):
    global gdf_pontos

    ponto = Point(lon, lat)

    if gdf_pontos is None:
        gdf_pontos = gpd.GeoDataFrame(columns=[
            'geometry', 'Code', 'valor', 'unidade', 'maduro_kg',
            'coletado', 'latitude', 'longitude', 'metodo'
        ], geometry='geometry', crs="EPSG:4326")

    novo_ponto = {
        'geometry': ponto,
        'Code': gerar_codigo(),
        'valor': 0,
        'unidade': unidade_dropdown.value,
        'maduro_kg': 0,
        'coletado': False,
        'latitude': lat,
        'longitude': lon,
        'metodo': metodo
    }

    gdf_pontos = gpd.GeoDataFrame(pd.concat([gdf_pontos, pd.DataFrame([novo_ponto])]), crs="EPSG:4326")

    # Adiciona ao mapa (cor laranja para pontos n√£o coletados)
    marker = geemap.geojson_to_ee({
        "type": "Feature",
        "geometry": mapping(ponto),
        "properties": {"status": "N√£o coletado"}
    })
    Map.add_layer(marker, {'color': 'orange'}, f"Ponto {len(gdf_pontos)}")

    # Ativa bot√µes
    btn_inserir_prod.disabled = False
    btn_inserir_voz.disabled = False
    btn_inserir_manual.disabled = False
    btn_salvar_pontos.disabled = False

    return len(gdf_pontos) - 1  # Retorna o √≠ndice do ponto adicionado

btn_adicionar_pontos.on_click(toggle_pontos_options)

# -----------------------------
# FUN√á√ïES DE INSER√á√ÉO POR VOZ
# -----------------------------
def capturar_por_voz_com_gps(b):
    if not btn_ativar_gps.value:
        with output:
            print("‚ö†Ô∏è Ative o GPS primeiro!")
        return

    # Simula√ß√£o de GPS - substituir por coordenadas reais
    lat, lon = -15.123, -55.456

    with output:
        output.clear_output()
        print("üé§ Aguarde... ouvindo valor de produtividade")

    try:
        recognizer = sr.Recognizer()
        with sr.Microphone() as source:
            recognizer.adjust_for_ambient_noise(source)
            audio = recognizer.listen(source, timeout=5)

        texto = recognizer.recognize_google(audio, language='pt-BR')
        valor = float(texto.replace(',', '.'))

        # Adiciona o ponto com o valor capturado
        adicionar_ponto(lat, lon, "voz_gps")

        # Atualiza o √∫ltimo ponto adicionado
        gdf_pontos.at[len(gdf_pontos)-1, 'valor'] = valor
        gdf_pontos.at[len(gdf_pontos)-1, 'coletado'] = True
        gdf_pontos.at[len(gdf_pontos)-1, 'maduro_kg'] = converter_para_kg(valor, unidade_dropdown.value)

        with output:
            print(f"‚úÖ Valor '{valor}' inserido no Ponto {len(gdf_pontos)}")

    except Exception as e:
        with output:
            print(f"‚ùå Erro ao reconhecer voz: {str(e)}")

btn_inserir_voz_gps.on_click(capturar_por_voz_com_gps)
btn_inserir_voz.on_click(capturar_por_voz_com_gps)

# -----------------------------
# FUN√á√ïES DE INSER√á√ÉO DE PRODUTIVIDADE
# -----------------------------
def inserir_produtividade(b):
    if gdf_pontos is None or gdf_pontos.empty:
        with output:
            print("‚ö†Ô∏è Nenhum ponto dispon√≠vel para inser√ß√£o de dados.")
        return

    # Cria interface para edi√ß√£o dos pontos
    pontos_widgets = []

    for idx, row in gdf_pontos.iterrows():
        hbox = HBox([
            HTML(f"<b>Ponto {idx+1}</b> (Lat: {row['latitude']:.5f}, Lon: {row['longitude']:.5f})"),
            FloatText(value=row['valor'], description='Valor:'),
            Dropdown(options=['kg', 'latas', 'litros'], value=row['unidade'], description='Unidade:'),
            Checkbox(value=row['coletado'], description='Coletado')
        ])
        pontos_widgets.append(hbox)

    def salvar_produtividade(btn):
        for idx, widget in enumerate(pontos_widgets):
            valor = widget.children[1].value
            unidade = widget.children[2].value
            coletado = widget.children[3].value

            gdf_pontos.at[idx, 'valor'] = valor
            gdf_pontos.at[idx, 'unidade'] = unidade
            gdf_pontos.at[idx, 'coletado'] = coletado
            gdf_pontos.at[idx, 'maduro_kg'] = converter_para_kg(valor, unidade)

        # Atualiza marcadores no mapa
        for idx, row in gdf_pontos.iterrows():
            for layer in Map.layers:
                if layer.name == f"Ponto {idx+1}":
                    Map.remove_layer(layer)

            color = 'green' if row['coletado'] else 'orange'
            marker = geemap.geojson_to_ee({
                "type": "Feature",
                "geometry": mapping(row['geometry']),
                "properties": {"status": "Coletado" if row['coletado'] else "N√£o coletado"}
            })
            Map.add_layer(marker, {'color': color}, f"Ponto {idx+1}")

        with output:
            output.clear_output()
            print("‚úÖ Dados de produtividade salvos com sucesso!")

    salvar_btn = Button(description='üíæ Salvar Todos', button_style='success')
    salvar_btn.on_click(salvar_produtividade)

    with output:
        output.clear_output()
        display(VBox(pontos_widgets + [salvar_btn]))

btn_inserir_prod.on_click(inserir_produtividade)

# -----------------------------
# FUN√á√ïES DE EXPORTA√á√ÉO
# -----------------------------
def salvar_pontos(b):
    """Prepara os dados para exporta√ß√£o"""
    global gdf_pontos

    if gdf_pontos is None or gdf_pontos.empty:
        with output:
            print("‚ö†Ô∏è Nenhum ponto para salvar!")
        return

    # Garante que maduro_kg est√° calculado
    gdf_pontos['maduro_kg'] = gdf_pontos.apply(
        lambda row: converter_para_kg(row['valor'], row['unidade']),
        axis=1
    )

    with output:
        print("‚úÖ Dados dos pontos preparados para exporta√ß√£o!")

btn_salvar_pontos.on_click(salvar_pontos)

# exportar dados
def exportar_dados(b):
    """Exporta os dados no formato compat√≠vel com o c√≥digo de processamento"""
    global densidade_plantas, produtividade_media

    output.clear_output()

    # Exportar par√¢metros como JSON
    parametros = {
        'densidade_pes_ha': densidade_plantas,
        'produtividade_media_sacas_ha': produtividade_media
    }

    parametros_path = '/content/parametros_area.json'
    with open(parametros_path, 'w') as f:
        json.dump(parametros, f)

    with output:
        print(f"‚úÖ Par√¢metros da √°rea exportados: {parametros_path}")
        print(f"- Densidade: {densidade_plantas} plantas/ha")
        print(f"- Produtividade m√©dia: {produtividade_media} sacas/ha")

    if gdf_poligono is not None:
        poligono_path = '/content/area_poligono.gpkg'
        gdf_poligono.to_file(poligono_path, driver='GPKG')

        if gdf_pontos is not None and not gdf_pontos.empty:
            pontos_path = '/content/pontos_produtividade.gpkg'
            gdf_pontos.to_file(pontos_path, driver='GPKG')
            with output:
                print(f"‚úÖ Arquivo do pol√≠gono exportado: {poligono_path}")
                print(f"‚úÖ Arquivo de pontos exportado: {pontos_path}")
                print(f"Total de pontos exportados: {len(gdf_pontos)}")
        else:
            with output:
                print(f"‚úÖ Arquivo do pol√≠gono exportado: {poligono_path}")
                print("‚ÑπÔ∏è Nenhum ponto de produtividade foi adicionado para exporta√ß√£o")
    else:
        with output:
            print("‚ö†Ô∏è Nenhuma √°rea desenhada para exportar")

    if gdf_poligono_total is not None:
        poligono_total_path = '/content/area_total_poligono.gpkg'
        gdf_poligono_total.to_file(poligono_total_path, driver='GPKG')
        with output:
            print(f"‚úÖ Pol√≠gono total exportado: {poligono_total_path}")
    else:
        with output:
            print("‚ö†Ô∏è Nenhum pol√≠gono total dispon√≠vel para exportar.")

# Conectar bot√£o
exportar_btn.on_click(exportar_dados)

# -----------------------------
# EXIBIR APLICATIVO
# -----------------------------
display(app_layout, output)
with output:
    print("üìå Instru√ß√µes completas:")
    print("1. Desenhe uma √°rea ou carregue um arquivo")
    print("2. Clique em 'Adicionar Pontos' e depois no mapa para criar pontos manualmente")
    print("3. Use 'Inserir Produtividade' para adicionar dados aos pontos")
    print("4. Exporte os dados quando terminar")

## PROCESSAR DADOS

import ee
import geemap
import pandas as pd
from datetime import datetime

# exportar os arquivos GPKG
if gdf_poligono is not None:
    # Exportar pol√≠gono
    poligono_path = '/content/area_poligono.gpkg'
    gdf_poligono.to_file(poligono_path, driver='GPKG')
    print(f"\n‚úÖ Arquivo do pol√≠gono exportado: {poligono_path}")

    # Verificar e exportar pontos se existirem
    if gdf_pontos is not None and not gdf_pontos.empty:
        pontos_path = '/content/pontos_produtividade.gpkg'
        gdf_pontos.to_file(pontos_path, driver='GPKG')
        print(f"‚úÖ Arquivo de pontos exportado: {pontos_path}")
        print(f"Total de pontos exportados: {len(gdf_pontos)}")
    else:
        print("\n‚ÑπÔ∏è Nenhum ponto de produtividade foi adicionado para exporta√ß√£o")
else:
    print("\n‚ö†Ô∏è Nenhuma √°rea definida para exporta√ß√£o")

arquivo_poligono = '/content/area_poligono.gpkg'
arquivo_pontos = '/content/pontos_produtividade.gpkg'

# Leitura dos arquivos
gdf_poligono = gpd.read_file(arquivo_poligono)
gdf_pontos = gpd.read_file(arquivo_pontos)

# Garantir que o CRS esteja em metros
if gdf_poligono.crs.is_geographic:
    gdf_poligono = gdf_poligono.to_crs(epsg=32723)

# C√°lculo da √°rea total em hectares
area_ha = gdf_poligono.geometry.area.sum() / 10000
print(f"\n√Årea total calculada a partir do pol√≠gono carregado: {area_ha:.2f} hectares")

# Mostrar conte√∫do do GeoDataFrame de pontos
print("\nDados do GeoDataFrame de pontos:")
display(gdf_pontos.head())

# Mostrar a contagem de fei√ß√µes e os nomes das colunas
print(f"\nPol√≠gono cont√©m {len(gdf_poligono)} fei√ß√µes e as colunas: {list(gdf_poligono.columns)}")
print(f"Pontos cont√™m {len(gdf_pontos)} fei√ß√µes e as colunas: {list(gdf_pontos.columns)}")

# Converter para objetos do Earth Engine
poligono = geemap.gdf_to_ee(gdf_poligono)
pontos = geemap.gdf_to_ee(gdf_pontos)

# Intervalo de datas da sele√ß√£o de imagens do sat√©lite Sentinel-2
data_inicio = '2023-08-01'
data_fim = '2024-05-31'

data_inicio_ee = ee.Date(data_inicio)
data_fim_ee = ee.Date(data_fim)

# Calcular √≠ndices espectrais
def calcular_indices(image):
    ndvi = image.normalizedDifference(['B8', 'B4']).rename('NDVI')
    ndre = image.normalizedDifference(['B8', 'B5']).rename('NDRE')
    ccci = ndre.divide(ndvi).rename('CCCI')
    gndvi = image.normalizedDifference(['B8', 'B3']).rename('GNDVI')
    ndmi = image.normalizedDifference(['B8', 'B11']).rename('NDMI')
    msavi2 = image.expression(
        '(2 * NIR + 1 - sqrt((2 * NIR + 1) ** 2 - 8 * (NIR - RED))) / 2', {
            'NIR': image.select('B8'),
            'RED': image.select('B4')
        }).rename('MSAVI2')
    nbr = image.normalizedDifference(['B8', 'B12']).rename('NBR')
    twi2 = image.normalizedDifference(['B9', 'B8']).rename('TWI2')
    ndwi = image.normalizedDifference(['B3', 'B8']).rename('NDWI')
    return image.addBands([ndvi, ndre, ccci, gndvi, ndmi, msavi2, nbr, twi2, ndwi])

# Filtrar cole√ß√£o Sentinel-2
colecao = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED') \
    .filterBounds(poligono.geometry()) \
    .filterDate(data_inicio_ee, data_fim_ee) \
    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 5)) \
    .map(calcular_indices)

# Validar imagens com dados
def imagem_valida(img):
    count = img.select(0).reduceRegion(
        reducer=ee.Reducer.count(),
        geometry=poligono,
        scale=10,
        maxPixels=1e9
    ).values().get(0)
    return img.set('valida', ee.Number(count).gt(0))

colecao_com_valida = colecao.map(imagem_valida)
imagens_validas = colecao_com_valida.filter(ee.Filter.eq('valida', 1))

def add_formatted_date(img):
    date_str = ee.Date(img.get('system:time_start')).format('YYYY-MM-dd')
    return img.set('formatted_date', date_str)

imagens_validas = imagens_validas.map(add_formatted_date)
distinct_dates = imagens_validas.distinct(['formatted_date'])
imagens_validas = ee.ImageCollection(distinct_dates)

datas = imagens_validas.aggregate_array('formatted_date').getInfo()

if datas:
    print(f"{len(datas)} imagens v√°lidas:")
    for data in sorted(datas):
        print(f"- {data}")
else:
    print("\nNenhuma imagem v√°lida encontrada com dados na √°rea especificada.")

# Definir os √≠ndices que ser√£o extra√≠dos
nomes_indices = ['NDVI', 'NDRE', 'CCCI', 'GNDVI', 'NDMI', 'MSAVI2', 'NBR', 'TWI2', 'NDWI']

# Fun√ß√£o para extrair estat√≠sticas por ponto
def extrair_estatisticas_ponto_imagem(imagem, feature_ponto, nomes_indices):
    buffer = feature_ponto.geometry().buffer(5)  # Buffer de 5 metros
    data_img = ee.Date(imagem.get('system:time_start')).format('yyyyMMdd')
    resultado_img = ee.Dictionary({})

    for indice in nomes_indices:
        banda = imagem.select(indice)
        reducao = banda.reduceRegion(
            reducer=ee.Reducer.min().combine(
                reducer2=ee.Reducer.mean(), sharedInputs=True
            ).combine(
                reducer2=ee.Reducer.max(), sharedInputs=True
            ),
            geometry=buffer,
            scale=10,
            maxPixels=1e8
        )

        valor_min = reducao.get(indice + '_min')
        valor_mean = reducao.get(indice + '_mean')
        valor_max = reducao.get(indice + '_max')

        chave_min = ee.String(data_img).cat('_').cat(indice).cat('_min')
        chave_mean = ee.String(data_img).cat('_').cat(indice).cat('_mean')
        chave_max = ee.String(data_img).cat('_').cat(indice).cat('_max')

        resultado_img = ee.Dictionary(ee.Algorithms.If(valor_min, resultado_img.set(chave_min, valor_min), resultado_img))
        resultado_img = ee.Dictionary(ee.Algorithms.If(valor_mean, resultado_img.set(chave_mean, valor_mean), resultado_img))
        resultado_img = ee.Dictionary(ee.Algorithms.If(valor_max, resultado_img.set(chave_max, valor_max), resultado_img))

    return ee.Feature(feature_ponto.geometry(), resultado_img)

# Processar todos os pontos
def processar_ponto(ponto, colecao_imagens, lista_indices):
    def extrair_para_cada_imagem(imagem):
        return extrair_estatisticas_ponto_imagem(imagem, ponto, lista_indices)

    resultados_por_imagem = colecao_imagens.map(extrair_para_cada_imagem)

    def combinar_props(feature, acumulador):
        return ee.Dictionary(acumulador).combine(ee.Feature(feature).toDictionary(), overwrite=True)

    propriedades_combinadas = ee.Dictionary(
        resultados_por_imagem.iterate(combinar_props, ee.Dictionary({}))
    )

    return ee.Feature(ponto.geometry(), propriedades_combinadas)

# Executar processamento
pontos_com_indices = pontos.map(lambda pt: processar_ponto(pt, imagens_validas, nomes_indices))

# Converter para GeoDataFrame em mem√≥ria
gdf_resultado = geemap.ee_to_gdf(pontos_com_indices)

# Adicionar coluna de produtividade observada
gdf_resultado['maduro_kg'] = gdf_pontos['maduro_kg'].values

# Separar vers√£o sem geometria (em mem√≥ria, sem salvar)
df_sem_geometria = gdf_resultado.drop(columns=['geometry'])

csv_limpo = '/content/indices_23_24_limpo.csv'
colunas_remover = ['geometry']
gdf_resultado.drop(columns=[col for col in colunas_remover if col in gdf_resultado.columns]) \
              .to_csv(csv_limpo, index=False)
print(f"Arquivo salvo sem geometria: {csv_limpo}")

# Mostrar os dados limpos
print("Dados do arquivo:")
print(pd.read_csv(csv_limpo))

import pandas as pd
import numpy as np
from scipy.stats import shapiro, pearsonr, spearmanr
import matplotlib.pyplot as plt
import seaborn as sns

# 1. Carregar os dados
df = pd.read_csv('/content/indices_23_24_limpo.csv')

# 2. Selecionar colunas de interesse
colunas_indices = [col for col in df.columns if any(x in col for x in ['NDVI', 'NDRE', 'CCCI', 'SAVI', 'GNDVI', 'NDMI', 'MSAVI2', 'NBR', 'TWI2', 'NDWI'])]
colunas_analise = ['maduro_kg'] + colunas_indices

# 3. Teste de Shapiro-Wilk (normalidade)
resultados_normalidade = []
for coluna in colunas_analise:
    stat, p = shapiro(df[coluna])
    normal = 'Sim' if p > 0.05 else 'N√£o'
    resultados_normalidade.append({'Vari√°vel': coluna, 'p-valor': p, 'Normal': normal})

df_normalidade = pd.DataFrame(resultados_normalidade)
print("\nResultados do Teste de Normalidade (Shapiro-Wilk):")
print(df_normalidade.sort_values('p-valor'))

# 4. Propor√ß√£o de vari√°veis normais
proporcao_normal = df_normalidade['Normal'].value_counts(normalize=True).get('Sim', 0)
print(f"\nPropor√ß√£o de vari√°veis normais: {proporcao_normal:.1%}")

# 5. Escolha do m√©todo de correla√ß√£o
if proporcao_normal > 0.5:
    metodo = 'pearson'
    print("\nUsando correla√ß√£o de Pearson (maioria normal)")
else:
    metodo = 'spearman'
    print("\nUsando correla√ß√£o de Spearman (maioria n√£o-normal)")

# 6. Matriz de correla√ß√£o e p-valores
corr_matrix = df[colunas_analise].corr(method=metodo.lower())

# Calcular p-valores se Pearson
if metodo == 'pearson':
    p_values = pd.DataFrame(np.zeros((len(colunas_analise), len(colunas_analise))),
                            columns=colunas_analise, index=colunas_analise)
    for i in colunas_analise:
        for j in colunas_analise:
            if i != j:
                _, p_val = pearsonr(df[i], df[j])
                p_values.loc[i, j] = p_val
            else:
                p_values.loc[i, j] = np.nan

# 7. Top 5 correla√ß√µes com maduro_kg
correlacoes_maduro = corr_matrix['maduro_kg'].drop('maduro_kg')
melhores_5 = correlacoes_maduro.abs().sort_values(ascending=False).head(5)

print("\nTop 5 √≠ndices com maior correla√ß√£o (absoluta) com produtividade (maduro_kg):")
for idx, valor in melhores_5.items():
    if metodo == 'pearson':
        p_val = p_values.loc['maduro_kg', idx]
        print(f"- {idx}: {valor:.3f} ({'positiva' if corr_matrix.loc['maduro_kg', idx] > 0 else 'negativa'}), p-valor: {p_val:.4f}")
    else:
        print(f"- {idx}: {valor:.3f} ({'positiva' if corr_matrix.loc['maduro_kg', idx] > 0 else 'negativa'})")

# 8. Explica√ß√£o did√°tica sobre correla√ß√£o
print("\n=== üìò Interpreta√ß√£o das Correla√ß√µes ===")
print("""
üîπ Correla√ß√£o de Pearson:
    - Mede a rela√ß√£o linear entre duas vari√°veis num√©ricas.
    - Pressup√µe que os dados sejam normalmente distribu√≠dos.
    - Varia de -1 a 1:
        + 1 ‚Üí correla√ß√£o perfeita positiva
        0 ‚Üí nenhuma correla√ß√£o
        -1 ‚Üí correla√ß√£o perfeita negativa
    - Exemplo: um valor de 0.75 indica que quando uma vari√°vel aumenta, a outra tende a aumentar tamb√©m.

üîπ Correla√ß√£o de Spearman:
    - Mede a rela√ß√£o monot√¥nica (n√£o necessariamente linear) entre duas vari√°veis.
    - Baseia-se na ordena√ß√£o dos dados (ranks).
    - N√£o exige distribui√ß√£o normal.
    - √ötil quando os dados possuem outliers ou rela√ß√µes n√£o lineares.

üîπ p-valor (apenas Pearson no script):
    - Indica a signific√¢ncia estat√≠stica da correla√ß√£o.
    - p < 0.05 ‚Üí correla√ß√£o estatisticamente significativa (n√≠vel de confian√ßa de 95%).

üîπ Como interpretar a for√ßa da correla√ß√£o:
    - 0.00 a 0.30 ‚Üí fraca
    - 0.31 a 0.50 ‚Üí moderada
    - 0.51 a 0.70 ‚Üí forte
    - 0.71 a 0.90 ‚Üí muito forte
    - acima de 0.90 ‚Üí quase perfeita

‚úÖ Dica:
    - CorrelacÃßoÃÉes naÃÉo implicam causalidade.
    - Use a an√°lise de correla√ß√£o como **etapa explorat√≥ria**, para saber se os dados analisados se correlacionam bem de alguma forma, n√£o como prova de rela√ß√£o causal. Boas correla√ß√µes negativas (pr√≥ximo de -1) tamb√©m podem indicar tend√™ncias dos dados.
""")

from sklearn.neural_network import MLPRegressor
from sklearn.svm import SVR
from xgboost import XGBRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import Ridge, Lasso, ElasticNet
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.inspection import permutation_importance
import pandas as pd
import numpy as np
import warnings
import joblib
from tqdm import tqdm

# Configura√ß√µes iniciais
warnings.filterwarnings('ignore')
np.random.seed(42)
NUM_EXECUCOES = 20

# 1. Carregar e preparar os dados
df = pd.read_csv("/content/indices_23_24_limpo.csv")
X = df.drop(columns=['maduro_kg'])
y = df['maduro_kg']

# 2. Definir os modelos
modelos = {
    "MLP": MLPRegressor(hidden_layer_sizes=(50, 50), activation='relu', solver='adam',
                        max_iter=2000, early_stopping=True, random_state=42),
    "SVR": SVR(kernel='rbf', C=1.0, epsilon=0.1),
    "XGBoost": XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42),
    "RandomForest": RandomForestRegressor(n_estimators=100, random_state=42),
    "GradientBoosting": GradientBoostingRegressor(n_estimators=100, random_state=42),
    "AdaBoost": AdaBoostRegressor(n_estimators=100, random_state=42),
    "DecisionTree": DecisionTreeRegressor(random_state=42),
    "KNN": KNeighborsRegressor(n_neighbors=5),
    "Ridge": Ridge(alpha=1.0, random_state=42),
    "Lasso": Lasso(alpha=0.1, random_state=42),
    "ElasticNet": ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42)
}
modelos_escalonados = ["MLP", "SVR", "KNN", "Ridge", "Lasso", "ElasticNet"]

# 3. Avalia√ß√£o dos modelos
resultados = {nome: [] for nome in modelos}

print(f"Executando {len(modelos)} modelos {NUM_EXECUCOES} vezes cada...\n")
for i in tqdm(range(NUM_EXECUCOES), desc="Progresso Geral"):
    try:
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=i)
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)

        for nome, modelo in modelos.items():
            if nome in modelos_escalonados:
                X_tr, X_te = X_train_scaled, X_test_scaled
            else:
                X_tr, X_te = X_train, X_test

            try:
                modelo.fit(X_tr, y_train)
                y_pred = modelo.predict(X_te)

                resultados[nome].append({
                    'execucao': i + 1,
                    'r2': r2_score(y_test, y_pred),
                    'rmse': np.sqrt(mean_squared_error(y_test, y_pred)),
                    'modelo': nome,
                    'convergiu': True
                })
            except Exception as e:
                print(f"\nErro no modelo {nome} (execu√ß√£o {i+1}): {str(e)}")
                resultados[nome].append({
                    'execucao': i + 1,
                    'r2': None,
                    'rmse': None,
                    'modelo': nome,
                    'convergiu': False
                })
    except Exception as e:
        print(f"\nErro na execu√ß√£o {i + 1}: {str(e)}")
        continue

# 4. An√°lise dos resultados (MODIFICADO)
def analisar_resultados(resultados, nome_modelo):
    df_resultados = pd.DataFrame(resultados[nome_modelo])
    df_validos = df_resultados[df_resultados['convergiu']]

    if not df_validos.empty:
        print(f"\n=== Resultados Individuais {nome_modelo} ===")
        print(df_validos[['execucao', 'r2', 'rmse']])  # Mostra valores individuais

        melhor_idx = df_validos['r2'].idxmax()
        melhor = df_validos.loc[melhor_idx]
        print(f"\nMelhor execu√ß√£o ({nome_modelo}): R¬≤ = {melhor['r2']:.4f}, RMSE = {melhor['rmse']:.4f}")
        return melhor
    else:
        print(f"\n{nome_modelo}: Nenhum modelo convergiu.")
        return None

# Criar o dicion√°rio melhores_modelos antes de us√°-lo
melhores_modelos = {nome: analisar_resultados(resultados, nome) for nome in modelos}  # Esta linha estava faltando

# 5. Salvar os melhores modelos
for nome, melhor in melhores_modelos.items():
    if melhor is not None:
        joblib.dump(modelos[nome], f'melhor_{nome.lower()}.pkl')

print("\nModelos otimizados foram salvos.")

# 6. Identificar o melhor modelo global (MODIFICADO)
df_melhores_execucoes = pd.DataFrame([
    {"modelo": nome, "r2": melhor["r2"], "rmse": melhor["rmse"]}
    for nome, melhor in melhores_modelos.items() if melhor is not None
])

melhor_modelo_nome = df_melhores_execucoes.loc[df_melhores_execucoes["r2"].idxmax()]["modelo"]  # Corrigido "modelo"
melhor_modelo = modelos[melhor_modelo_nome]

print(f"\nMelhor modelo global: {melhor_modelo_nome}")
print(f"Melhor R¬≤: {df_melhores_execucoes.loc[df_melhores_execucoes['modelo'] == melhor_modelo_nome]['r2'].values[0]:.4f}")  # Corrigir para 'modelo'
print(f"Melhor RMSE: {df_melhores_execucoes.loc[df_melhores_execucoes['modelo'] == melhor_modelo_nome]['rmse'].values[0]:.4f}")  # Corrigido

# 7. Import√¢ncia das features
if melhor_modelo_nome in modelos_escalonados:
    X_for_importance = StandardScaler().fit_transform(X)
else:
    X_for_importance = X

result_importance = permutation_importance(melhor_modelo, X_for_importance, y, n_repeats=10, random_state=42)
importancia_indices = pd.DataFrame({
    "√çndice": X.columns,
    "Import√¢ncia": result_importance.importances_mean
}).sort_values(by="Import√¢ncia", ascending=False)

top5_indices = importancia_indices.head(5)
top5_indices["Porcentagem"] = (top5_indices["Import√¢ncia"] / top5_indices["Import√¢ncia"].sum()) * 100

print("\nOs 5 √≠ndices espectrais mais importantes para o modelo selecionado:")
print(top5_indices)

# 8. Reproduzir a melhor execu√ß√£o e fazer predi√ß√£o apenas no conjunto de TESTE
melhor_execucao = resultados[melhor_modelo_nome][pd.DataFrame(resultados[melhor_modelo_nome])['r2'].idxmax()]
random_state_melhor = melhor_execucao['execucao'] - 1

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_state_melhor)

if melhor_modelo_nome in modelos_escalonados:
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    melhor_modelo.fit(X_train_scaled, y_train)
    y_pred = melhor_modelo.predict(X_test_scaled)
else:
    melhor_modelo.fit(X_train, y_train)
    y_pred = melhor_modelo.predict(X_test)

# 9. Avalia√ß√£o estat√≠stica com TESTE
def avaliacao_estatistica(y_real, y_pred):
    r2 = r2_score(y_real, y_pred)
    rmse = np.sqrt(mean_squared_error(y_real, y_pred))
    residuals = y_real - y_pred
    sst = np.sum((y_real - np.mean(y_real))**2)
    sse = np.sum(residuals**2)
    rmse_relativo = (rmse / np.mean(y_real)) * 100
    bias = np.mean(residuals)
    bias_relativo = (bias / np.mean(y_real)) * 100
    return {
        'R¬≤': r2,
        'RMSE': rmse,
        'RMSE Relativo (%)': rmse_relativo,
        'Bias': bias,
        'Bias Relativo (%)': bias_relativo
    }

metricas = avaliacao_estatistica(y_test, y_pred)

print("\n=== Avalia√ß√£o com Dados de TESTE ===")
for k, v in metricas.items():
    print(f"{k}: {v:.4f}" if '(%' not in k else f"{k}: {v:.2f}%")

# 10. Tabela com dados de TESTE
df_comparativo = pd.DataFrame({
    'Produtividade_Real': y_test,
    'Produtividade_Predita': y_pred,
    'Res√≠duo': y_test - y_pred
})
df_comparativo['Erro_Relativo'] = (df_comparativo['Res√≠duo'] / df_comparativo['Produtividade_Real']) * 100

print("\nTabela Comparativa (Apenas dados de TESTE):")
print(df_comparativo.sort_values('Produtividade_Real').head(10).to_string(index=False))

# Exportar top5 √≠ndices
top5_indices.to_csv('/content/top5_indices.csv', index=False)
# === Script adicional para salvar o melhor modelo como 'melhor_modelo.pkl' ===

# Verifica se o modelo j√° foi identificado corretamente
if 'melhor_modelo' in locals() and melhor_modelo is not None:
    try:
        # Adiciona ao modelo a informa√ß√£o das features usadas
        if hasattr(melhor_modelo, 'feature_names_in_'):
            pass  # j√° est√° presente
        else:
            melhor_modelo.feature_names_in_ = X.columns.to_numpy()

        joblib.dump(melhor_modelo, 'melhor_modelo.pkl')
        print("\n‚úÖ Melhor modelo global salvo com sucesso como 'melhor_modelo.pkl'.")
    except Exception as e:
        print(f"\n‚ö†Ô∏è Erro ao salvar o melhor modelo: {str(e)}")
else:
    print("\n‚ö†Ô∏è Melhor modelo n√£o encontrado no ambiente.")

# 11. Explica√ß√£o did√°tica das m√©tricas de avalia√ß√£o
print("\n=== üìò Interpreta√ß√£o das M√©tricas ===")
print("""
üîπ R¬≤ (Coeficiente de Determina√ß√£o):
    - Mede o quanto da variabilidade dos dados reais √© explicada pelo modelo.
    - Varia de 0 a 1. Quanto mais pr√≥ximo de 1, melhor o desempenho.
    - Exemplo: R¬≤ = 0.85 indica que 85% da variabilidade dos dados √© explicada pelo modelo.

üîπ RMSE (Root Mean Squared Error):
    - Erro m√©dio quadr√°tico da predi√ß√£o. √â sens√≠vel a grandes erros.
    - Mede, em unidades reais (ex: kg), o desvio m√©dio entre o valor real e o previsto.
    - Quanto mais pr√≥ximo de zero o RMSE, melhor.

üîπ RMSE Relativo (%):
    - RMSE em rela√ß√£o √† m√©dia dos valores reais (em percentual).
    - Permite comparar erros entre diferentes contextos ou culturas agr√≠colas.
    - Exemplo: RMSE relativo de 12% significa que o erro m√©dio representa 12% da produtividade m√©dia.

üîπ Bias (Vi√©s):
    - Indica se o modelo tende a superestimar (bias negativo) ou subestimar (bias positivo) os valores.
    - Idealmente, deve ser pr√≥ximo de zero.

üîπ Bias Relativo (%):
    - Bias expresso em rela√ß√£o √† m√©dia dos valores reais.
    - Ajuda a avaliar a tend√™ncia sistem√°tica do erro em termos percentuais.

‚úÖ Recomenda√ß√µes:
    - Busque R¬≤ alto (‚â• 0.75), RMSE e bias baixos.
    - Sempre avalie RMSE e bias relativos para entender o impacto em termos percentuais.
""")

# 11. Predi√ß√£o para todos os pontos usando o melhor modelo
print("\n=== Predi√ß√£o para todos os pontos ===")

# Preparar os dados completos
X_full = X  # J√° carregamos todos os dados anteriormente
y_full = y   # Todos os valores reais

# Verificar se precisamos escalonar os dados para o melhor modelo
if melhor_modelo_nome in modelos_escalonados:
    # Usar o mesmo scaler da melhor execu√ß√£o
    scaler = StandardScaler()
    X_full_scaled = scaler.fit_transform(X_full)  # Fit apenas para evitar warning, j√° que estamos usando todos os dados
    X_for_prediction = X_full_scaled
else:
    X_for_prediction = X_full

# Fazer as predi√ß√µes para todos os pontos
y_full_pred = melhor_modelo.predict(X_for_prediction)

# Criar dataframe com resultados completos
df_full_comparison = pd.DataFrame({
    'Ponto': df.index + 1,  # Assumindo que cada linha representa um ponto diferente
    'Produtividade_Real': y_full,
    'Produtividade_Predita': y_full_pred,
    'Res√≠duo': y_full - y_full_pred
})

# Calcular erro relativo
df_full_comparison['Erro_Relativo'] = (df_full_comparison['Res√≠duo'] / df_full_comparison['Produtividade_Real']) * 100

# Ordenar por produtividade real para melhor visualiza√ß√£o
df_full_comparison = df_full_comparison.sort_values('Produtividade_Real').reset_index(drop=True)

# Exibir o dataframe completo
print("\nTabela Comparativa para todos os 20 pontos:")
print(df_full_comparison.to_string(index=False))

# 12. Avalia√ß√£o estat√≠stica com TODOS os dados
metricas_full = avaliacao_estatistica(y_full, y_full_pred)

print("\n=== Avalia√ß√£o com TODOS os dados ===")
for k, v in metricas_full.items():
    print(f"{k}: {v:.4f}" if '(%' not in k else f"{k}: {v:.2f}%")

# 14. Salvar resultados completos em CSV
df_full_comparison.to_csv('resultados_completos_predicao.csv', index=False)
print("\nResultados completos salvos em 'resultados_completos_predicao.csv'")

import geopandas as gpd
import json

area_amostral = gpd.read_file('/content/area_poligono.gpkg')
area_amostral = area_amostral.to_crs(epsg=31983)
valor = area_amostral.geometry.area.sum() / 10000
print(f"valor = {valor:.2f} ha")

with open('/content/parametros_area.json') as f:
    parametros = json.load(f)
densidade = parametros.get('densidade_pes_ha')
produtividade = parametros.get('produtividade_media_sacas_ha')

print(f"densidade = {densidade} plantas/ha")
print(f"produtividade = {produtividade} sacas/ha")

area_total_ha = valor
densidade_pes_ha = densidade
sacas_ha = produtividade
kg_por_saca = 60

# Dados das amostras
total_pontos_amostrais = 20  # Total de pontos amostrais
pes_por_amostra = 5  # 5 p√©s por amostra

# C√°lculo do fator de convers√£o
area_por_ponto_ha = (1/densidade_pes_ha) * pes_por_amostra  # √Årea que cada ponto representa em hectares
print(f"Cada ponto amostral representa: {area_por_ponto_ha:.6f} ha")

# Criar dataframe com valores reais e preditos (usando o c√≥digo complementar anterior)
df_predicoes_completas = df_full_comparison.copy()
df_predicoes_completas = df_predicoes_completas.rename(columns={
    'Produtividade_Real': 'maduro_kg_real',
    'Produtividade_Predita': 'maduro_kg_predito'
})

# 1. C√°lculo com base nos valores REAIS de todos os 20 pontos
produtividade_real_kg_ha = df_predicoes_completas['maduro_kg_real'].mean() / area_por_ponto_ha
produtividade_real_sacas_ha = produtividade_real_kg_ha / kg_por_saca

# 2. C√°lculo com base nos valores PREDITOS de todos os 20 pontos
produtividade_predita_kg_ha = df_predicoes_completas['maduro_kg_predito'].mean() / area_por_ponto_ha
produtividade_predita_sacas_ha = produtividade_predita_kg_ha / kg_por_saca

# 3. Compara√ß√£o com a produtividade m√©dia conhecida
print("\n=== COMPARA√á√ÉO DE PRODUTIVIDADES (20 PONTOS) ===")
print(f"Produtividade m√©dia conhecida da √°rea: {sacas_ha} sacas/ha")
print(f"\nBaseado nas 20 amostras reais:")
print(f"- Produtividade: {produtividade_real_kg_ha:.2f} kg/ha")
print(f"- Equivalente a: {produtividade_real_sacas_ha:.2f} sacas/ha")
print(f"- Diferen√ßa: {produtividade_real_sacas_ha - sacas_ha:.2f} sacas/ha")

print(f"\nBaseado nas previs√µes do modelo {melhor_modelo_nome} para 20 pontos:")
print(f"- Produtividade: {produtividade_predita_kg_ha:.2f} kg/ha")
print(f"- Equivalente a: {produtividade_predita_sacas_ha:.2f} sacas/ha")
print(f"- Diferen√ßa: {produtividade_predita_sacas_ha - sacas_ha:.2f} sacas/ha")

# 4. C√°lculo da produ√ß√£o total estimada
producao_total_real_kg = produtividade_real_kg_ha * area_total_ha
producao_total_real_sacas = produtividade_real_sacas_ha * area_total_ha

producao_total_predita_kg = produtividade_predita_kg_ha * area_total_ha
producao_total_predita_sacas = produtividade_predita_sacas_ha * area_total_ha

print("\n=== PRODU√á√ÉO TOTAL ESTIMADA ===")
print(f"Produ√ß√£o total REAL para {area_total_ha} ha:")
print(f"- {producao_total_real_kg:.2f} kg")
print(f"- {producao_total_real_sacas:.2f} sacas")

print(f"\nProdu√ß√£o total PREDITA para {area_total_ha} ha:")
print(f"- {producao_total_predita_kg:.2f} kg")
print(f"- {producao_total_predita_sacas:.2f} sacas")

# 5. An√°lise de discrep√¢ncia
discrepancia_percentual_real = ((produtividade_real_sacas_ha - sacas_ha) / sacas_ha) * 100
discrepancia_percentual_predita = ((produtividade_predita_sacas_ha - sacas_ha) / sacas_ha) * 100

print("\n=== DISCREP√ÇNCIAS ===")
print(f"Discrep√¢ncia das amostras reais: {discrepancia_percentual_real:.2f}%")
print(f"Discrep√¢ncia do modelo {melhor_modelo_nome}: {discrepancia_percentual_predita:.2f}%")

# 6. Resumo estat√≠stico
print("\n=== RESUMO ESTAT√çSTICO ===")
print("Valores reais (20 pontos):")
print(df_predicoes_completas['maduro_kg_real'].describe())
print("\nValores preditos (20 pontos):")
print(df_predicoes_completas['maduro_kg_predito'].describe())

import pandas as pd
import geopandas as gpd
import numpy as np
from pyproj import CRS
import matplotlib.pyplot as plt
from scipy.interpolate import griddata
import folium
from folium.plugins import HeatMap

# 1. Dados j√° existentes: gdf_resultado (pontos com √≠ndices + produtividade) e gdf_poligono

# Extrair coordenadas dos pontos
gdf_resultado['Longitude'] = gdf_resultado.geometry.x
gdf_resultado['Latitude'] = gdf_resultado.geometry.y

# Criar coluna de identifica√ß√£o dos pontos
gdf_resultado['Ponto'] = gdf_resultado.index + 1

# Carregar as predi√ß√µes de produtividade
produtividade_df = pd.read_csv('/content/resultados_completos_predicao.csv')

# Mesclar as tabelas
dados_completos = pd.merge(gdf_resultado, produtividade_df, on='Ponto')

# Selecionar apenas colunas necess√°rias
dados_finais = dados_completos[['Ponto', 'Latitude', 'Longitude', 'Produtividade_Predita']]

# Salvar CSV consolidado
dados_finais.to_csv('/content/pontos_produtividade.csv', index=False)
print("Arquivo '/content/pontos_produtividade.csv' criado com sucesso!")

# Visualizar o CSV completo
df = pd.read_csv('/content/pontos_produtividade.csv')
pd.set_option('display.max_rows', None)
print(df)

!pip install -q pykrige
import geopandas as gpd
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from scipy.spatial import cKDTree
from scipy.interpolate import griddata, Rbf
from matplotlib.path import Path
try:
    from pykrige.ok import OrdinaryKriging
    KRIGING_AVAILABLE = True
except ImportError:
    KRIGING_AVAILABLE = False
    print("Aviso: pykrige n√£o est√° instalado. O m√©todo de krigagem n√£o estar√° dispon√≠vel.")

# 1. Reutilizar os dados j√° carregados e processados
# gdf_resultado: cont√©m pontos com coordenadas e 'maduro_kg'
# gdf_poligono: cont√©m o pol√≠gono da √°rea

# Criar novo GeoDataFrame apenas com o necess√°rio
gdf_pontos = gdf_resultado[['geometry', 'maduro_kg']].copy()
gdf_pontos = gpd.GeoDataFrame(gdf_pontos, geometry='geometry', crs="EPSG:4326").to_crs("EPSG:32722")
gdf_poligono_utm = gdf_poligono.to_crs("EPSG:32722")

# Extrair coordenadas e produtividade
xyz = np.array([
    [p.x, p.y, val] for p, val in zip(gdf_pontos.geometry, gdf_pontos['maduro_kg'])
])

# Criar grade regular
xmin, ymin, xmax, ymax = gdf_poligono_utm.total_bounds
res = 5  # resolu√ß√£o em metros
xi = np.arange(xmin, xmax, res)
yi = np.arange(ymin, ymax, res)
xi_grid, yi_grid = np.meshgrid(xi, yi)

# Fun√ß√£o para aplicar m√°scara do pol√≠gono
def apply_mask(zi, xi_grid, yi_grid, gdf_area):
    poly_path = Path(gdf_area.geometry.unary_union.exterior.coords)
    mask = np.array([
        poly_path.contains_point((x, y)) for x, y in zip(xi_grid.flatten(), yi_grid.flatten())
    ])
    zi_masked = np.full_like(zi, np.nan)
    zi_masked_flat = zi_masked.flatten()
    zi_masked_flat[mask] = zi.flatten()[mask]
    return zi_masked_flat.reshape(zi.shape)

# M√©todos de interpola√ß√£o
def idw_interpolation(xyz, xi, yi, power=2):
    tree = cKDTree(xyz[:, :2])
    distances, idx = tree.query(np.c_[xi.flatten(), yi.flatten()], k=5)
    weights = 1.0 / (distances**power)
    weights = np.where(np.isinf(weights), 0, weights)
    z = np.sum(weights * xyz[idx, 2], axis=1) / np.sum(weights, axis=1)
    return z.reshape(xi.shape)

def spline_interpolation(xyz, xi, yi):
    rbf = Rbf(xyz[:, 0], xyz[:, 1], xyz[:, 2], function='thin_plate')
    return rbf(xi, yi)

def nearest_interpolation(xyz, xi, yi):
    return griddata((xyz[:, 0], xyz[:, 1]), xyz[:, 2], (xi, yi), method='nearest')

def linear_interpolation(xyz, xi, yi):
    return griddata((xyz[:, 0], xyz[:, 1]), xyz[:, 2], (xi, yi), method='linear')

METHODS = {
    'idw': {'function': idw_interpolation, 'description': 'Inverse Distance Weighting'},
    'spline': {'function': spline_interpolation, 'description': 'Spline'},
    'nearest': {'function': nearest_interpolation, 'description': 'Vizinho Mais Pr√≥ximo'},
    'linear': {'function': linear_interpolation, 'description': 'Interpola√ß√£o Linear'}
}

# Adiciona krigagem se dispon√≠vel
if KRIGING_AVAILABLE:
    def kriging_interpolation(xyz, xi, yi):
        OK = OrdinaryKriging(
            xyz[:, 0], xyz[:, 1], xyz[:, 2],
            variogram_model='spherical',
            nlags=6,
            weight=True
        )
        zi, _ = OK.execute('grid', xi.flatten(), yi.flatten())
        return zi.reshape(xi.shape)

    METHODS['kriging'] = {'function': kriging_interpolation, 'description': 'Krigagem Ordin√°ria'}

# Escolha do m√©todo
METHOD = 'spline'  # Altere aqui se quiser testar outro

# Verifica se m√©todo est√° dispon√≠vel
if METHOD not in METHODS:
    raise ValueError(f"M√©todo '{METHOD}' n√£o dispon√≠vel. Use um dos: {', '.join(METHODS.keys())}")

# Aplica interpola√ß√£o
print(f"Usando m√©todo: {METHODS[METHOD]['description']}")
zi = METHODS[METHOD]['function'](xyz, xi_grid, yi_grid)
zi_masked = apply_mask(zi, xi_grid, yi_grid, gdf_poligono_utm)

# Plot do mapa de variabilidade
fig, ax = plt.subplots(figsize=(10, 8))
img = ax.imshow(
    zi_masked, extent=(xmin, xmax, ymin, ymax),
    origin='lower', cmap='YlGn', interpolation='nearest'
)
gdf_poligono_utm.boundary.plot(ax=ax, color='black', linewidth=1)
gdf_pontos.plot(ax=ax, color='red', markersize=20)

cbar = plt.colorbar(img, ax=ax, shrink=0.7)
cbar.set_label('Produtividade Observada (kg)')

# Rosa dos ventos e escala
ax.annotate('N', xy=(0.95, 0.95), xytext=(0.95, 0.95),
            xycoords='axes fraction', textcoords='axes fraction',
            ha='center', va='center', fontsize=12, fontweight='bold')
ax.annotate('', xy=(0.95, 0.92), xytext=(0.95, 0.95),
            xycoords='axes fraction', textcoords='axes fraction',
            arrowprops=dict(arrowstyle='->', lw=1.5))
ax.annotate('E', xy=(0.98, 0.92), xytext=(0.98, 0.92),
            xycoords='axes fraction', textcoords='axes fraction',
            ha='center', va='center', fontsize=10)
ax.annotate('S', xy=(0.95, 0.89), xytext=(0.95, 0.89),
            xycoords='axes fraction', textcoords='axes fraction',
            ha='center', va='center', fontsize=10)
ax.annotate('W', xy=(0.92, 0.92), xytext=(0.92, 0.92),
            xycoords='axes fraction', textcoords='axes fraction',
            ha='center', va='center', fontsize=10)

# Barra de escala
scale_bar_length = 100  # metros
scale_bar_text = f"{scale_bar_length} m"
ax.plot([0.85, 0.85 + scale_bar_length/(xmax-xmin)], [0.05, 0.05],
        color='black', linewidth=3, transform=ax.transAxes)
ax.text(0.85 + scale_bar_length/(2*(xmax-xmin)), 0.07, scale_bar_text,
        ha='center', va='bottom', transform=ax.transAxes, fontsize=10)

# T√≠tulos e cr√©ditos
ax.set_title(f'Variabilidade espacial da produtividade - {METHODS[METHOD]["description"]}')
ax.set_xlabel("UTM Easting (m)")
ax.set_ylabel("UTM Northing (m)")
ax.axis('on')

# Atualiza√ß√£o para exibir lat/lon
from pyproj import Transformer
transformer = Transformer.from_crs("EPSG:32722", "EPSG:4326", always_xy=True)
def format_coord(x, y):
    lon, lat = transformer.transform(x, y)
    return f"X={x:.1f}, Y={y:.1f} | Lon={lon:.5f}, Lat={lat:.5f}"
ax.format_coord = format_coord

# Texto inferior com SRC e cr√©dito
ax.text(0.5, -0.12, "SCR: WGS84 / 24S | July 27, 2025 | Cr√©dito: M√°rio Bittencourt",
        transform=ax.transAxes, ha='center', va='center', fontsize=9)

plt.tight_layout()
plt.show()

import ee
import geemap
import pandas as pd
import geopandas as gpd
import joblib
from sklearn.preprocessing import StandardScaler
import numpy as np
import warnings
warnings.filterwarnings('ignore')

# Par√¢metros
bandas = ['CCCI', 'NDMI', 'NDVI', 'GNDVI', 'NDWI','NBR','TWI2','NDRE','MSAVI2']
periodo_treinamento = ('2023-08-01', '2024-05-31')
periodo_predicao = ('2024-08-01', '2025-05-31')

# --- Fun√ß√£o para processar cole√ß√£o com filtro de nuvem adaptado ---
def processar_colecao(start_date, end_date, roi):
    # Define limite de nuvens conforme o per√≠odo
    if start_date == '2023-08-01' and end_date == '2024-05-31':
        limite_nuvens = 5
    elif start_date == '2024-08-01' and end_date == '2025-05-31':
        limite_nuvens = 20

    colecao_base = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED') \
        .filterBounds(roi) \
        .filterDate(start_date, end_date) \
        .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', limite_nuvens)) \
        .map(lambda img: img.addBands([
            # C√°lculo de todas as bandas necess√°rias
            img.normalizedDifference(['B8', 'B5']).divide(img.normalizedDifference(['B8', 'B4'])).rename('CCCI'),
            img.normalizedDifference(['B8', 'B11']).rename('NDMI'),
            img.normalizedDifference(['B3', 'B8']).rename('NDWI'),
            img.normalizedDifference(['B8', 'B12']).rename('NBR'),
            img.normalizedDifference(['B9', 'B8']).rename('TWI2'),
            img.normalizedDifference(['B8', 'B4']).rename('NDVI'),
            img.normalizedDifference(['B8', 'B5']).rename('NDRE'),
            img.expression(
            '(2 * NIR + 1 - sqrt((2 * NIR + 1) ** 2 - 8 * (NIR - RED))) / 2', {
                'NIR': img.select('B8'),
                'RED': img.select('B4')
            }).rename('MSAVI2'),
            img.normalizedDifference(['B8', 'B3']).rename('GNDVI')
        ])) \
        .map(lambda img: img.set('data', img.date().format('YYYY-MM-dd')))  # Adiciona atributo de data

    # Obter lista de datas √∫nicas
    datas_unicas = colecao_base.aggregate_array('data').distinct()

    # Para cada data √∫nica, pegar apenas a primeira imagem correspondente
    def imagem_unica_por_data(data):
        data = ee.String(data)
        imagens_na_data = colecao_base.filter(ee.Filter.eq('data', data))
        return ee.Image(imagens_na_data.first())

    colecao_filtrada = ee.ImageCollection(datas_unicas.map(imagem_unica_por_data))

    return colecao_filtrada

# --- [MANTIDAS TODAS AS FUN√á√ïES AUXILIARES ORIGINAIS] ---
def extrair_estatisticas_ponto_imagem(imagem, feature_ponto, nomes_indices):
    buffer = feature_ponto.geometry().buffer(5)
    resultado_img = ee.Dictionary({})

    for indice in nomes_indices:
        banda = imagem.select(indice)
        reducao = banda.reduceRegion(
            reducer=ee.Reducer.min().combine(
                reducer2=ee.Reducer.mean(), sharedInputs=True
            ).combine(
                reducer2=ee.Reducer.max(), sharedInputs=True
            ),
            geometry=buffer,
            scale=10,
            maxPixels=1e8
        )
        valor_min = reducao.get(indice + '_min')
        valor_mean = reducao.get(indice + '_mean')
        valor_max = reducao.get(indice + '_max')

        resultado_img = resultado_img.set(f"{indice}_min", valor_min)
        resultado_img = resultado_img.set(f"{indice}_mean", valor_mean)
        resultado_img = resultado_img.set(f"{indice}_max", valor_max)

    return ee.Feature(feature_ponto.geometry(), resultado_img)

def processar_ponto(ponto, colecao_imagens, lista_indices):
    def extrair_para_cada_imagem(imagem):
        return extrair_estatisticas_ponto_imagem(imagem, ponto, lista_indices)

    resultados_por_imagem = colecao_imagens.map(extrair_para_cada_imagem)

    def combinar_props(feature, acumulador):
        return ee.Dictionary(acumulador).combine(ee.Feature(feature).toDictionary(), overwrite=True)

    propriedades_combinadas = ee.Dictionary(
        resultados_por_imagem.iterate(combinar_props, ee.Dictionary({}))
    )

    return ee.Feature(ponto.geometry(), propriedades_combinadas)

def extrair_dados_min_mean_max(colecao, pontos_path, nomes_indices):
    gdf_pontos = gpd.read_file(pontos_path)
    pontos_ee = geemap.gdf_to_ee(gdf_pontos)
    pontos_processados = pontos_ee.map(lambda pt: processar_ponto(pt, colecao, nomes_indices))
    gdf_resultado = geemap.ee_to_gdf(pontos_processados)

    # Copiar colunas relevantes de produtividade do arquivo original (se houver)
    for col in gdf_pontos.columns:
        if col not in gdf_resultado.columns and col != 'geometry':
            gdf_resultado[col] = gdf_pontos[col]
    return gdf_resultado

# --- 1. Carregar √°rea de interesse e definir ROI ---
area_interesse = gpd.read_file('/content/area_poligono.gpkg')
roi = geemap.geopandas_to_ee(area_interesse)

# --- 2. Extrair dados de treinamento ---
colecao_treino = processar_colecao(*periodo_treinamento, roi)
dados_treino = extrair_dados_min_mean_max(colecao_treino, '/content/pontos_produtividade.gpkg', bandas)

# Preparar X e y para treino
X = dados_treino[[f"{b}_{stat}" for b in bandas for stat in ['min', 'mean', 'max']]]
y = dados_treino['maduro_kg']

# modelo treinado
modelo = joblib.load('/content/melhor_modelo.pkl')
print(f"\nModelo carregado - Features esperadas: {len(modelo.feature_names_in_)}")
modelo.fit(X, y)

# Salvar modelo treinado
joblib.dump(modelo, 'modelo_treino_sem_datas.pkl')

# --- 4. Extrair dados para predi√ß√£o ---
colecao_pred = processar_colecao(*periodo_predicao, roi)
dados_predicao = extrair_dados_min_mean_max(colecao_pred, '/content/pontos_produtividade.gpkg', bandas)

X_pred = dados_predicao[[f"{b}_{stat}" for b in bandas for stat in ['min', 'mean', 'max']]]

# --- 5. Fazer predi√ß√£o ---
y_pred = modelo.predict(X_pred)

dados_predicao['produtividade_kg'] = y_pred
dados_predicao['produtividade_sc/ha'] = dados_predicao['produtividade_kg'] * (1/60) * (1/0.0016)

print(f"Produtividade m√©dia prevista: {dados_predicao['produtividade_sc/ha'].mean():.2f} sacas/ha")

# Salvar resultados
dados_predicao.to_csv('/content/predicao_sem_datas.csv', index=False)
print("Resultados exportados para 'predicao_sem_datas.csv'")

# --- Verifica√ß√£o complementar ---
def verificar_colecao_imagens(colecao, nome_colecao):
    print(f"\n{'='*60}")
    print(f"VERIFICA√á√ÉO DE IMAGENS - {nome_colecao.upper()}".center(60))
    print(f"{'='*60}\n")

    total = colecao.size().getInfo()
    print(f"Total de imagens na cole√ß√£o {nome_colecao}: {total}")

    if total == 0:
        return

    def extrair_data(img):
        return img.set('data', img.date().format('YYYY-MM-dd'))

    colecao_datas = colecao.map(extrair_data)
    datas = colecao_datas.aggregate_array('data').getInfo()

    print("\nDatas das imagens dispon√≠veis:")
    for i, data in enumerate(sorted(datas), 1):
        print(f"{i}. {data}")

    print(f"\n{'='*60}\n")

# Verifica√ß√µes opcionais
verificar_colecao_imagens(colecao_treino, "Treinamento")
verificar_colecao_imagens(colecao_pred, "Predi√ß√£o")

import ee
import geemap
import pandas as pd
import geopandas as gpd
import joblib
import matplotlib.pyplot as plt
from sklearn.ensemble import GradientBoostingRegressor
import warnings
warnings.filterwarnings('ignore')

# 2. Par√¢metros do projeto
bandas = ['CCCI', 'NDMI', 'NDVI', 'GNDVI', 'NDWI','NBR','TWI2','NDRE','MSAVI2']
periodo_treinamento = ('2023-08-01', '2024-05-31')
periodo_predicao = ('2024-08-01', '2025-05-31')

# 3. Fun√ß√£o para processar a cole√ß√£o de imagens
def processar_colecao(start_date, end_date, roi):
    # Define limite de nuvens conforme o per√≠odo
    if start_date == '2023-08-01' and end_date == '2024-05-31':
        limite_nuvens = 5
    elif start_date == '2024-08-01' and end_date == '2025-05-31':
        limite_nuvens = 20

    colecao_base = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED') \
        .filterBounds(roi) \
        .filterDate(start_date, end_date) \
        .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', limite_nuvens)) \
        .map(lambda img: img.addBands([
            img.normalizedDifference(['B8', 'B5']).divide(img.normalizedDifference(['B8', 'B4'])).rename('CCCI'),
            img.normalizedDifference(['B8', 'B11']).rename('NDMI'),
            img.normalizedDifference(['B3', 'B8']).rename('NDWI'),
            img.normalizedDifference(['B8', 'B12']).rename('NBR'),
            img.normalizedDifference(['B9', 'B8']).rename('TWI2'),
            img.normalizedDifference(['B8', 'B4']).rename('NDVI'),
            img.normalizedDifference(['B8', 'B5']).rename('NDRE'),
            img.expression(
            '(2 * NIR + 1 - sqrt((2 * NIR + 1) ** 2 - 8 * (NIR - RED))) / 2', {
                'NIR': img.select('B8'),
                'RED': img.select('B4')
            }).rename('MSAVI2'),
            img.normalizedDifference(['B8', 'B3']).rename('GNDVI')
        ])) \
        .map(lambda img: img.set('data', img.date().format('YYYY-MM-dd')))  # Adiciona atributo de data

    # Obter lista de datas √∫nicas
    datas_unicas = colecao_base.aggregate_array('data').distinct()

    # Para cada data √∫nica, pegar apenas a primeira imagem correspondente
    def imagem_unica_por_data(data):
        data = ee.String(data)
        imagens_na_data = colecao_base.filter(ee.Filter.eq('data', data))
        return ee.Image(imagens_na_data.first())

    colecao_filtrada = ee.ImageCollection(datas_unicas.map(imagem_unica_por_data))

    return colecao_filtrada

# 4. Carregar √°rea de interesse
area_interesse = gpd.read_file('/content/area_poligono.gpkg')
roi = geemap.geopandas_to_ee(area_interesse)

# 5. Processar cole√ß√µes de imagens
colecao_treino = processar_colecao(*periodo_treinamento, roi)
colecao_pred = processar_colecao(*periodo_predicao, roi)

# 6. Fun√ß√£o para calcular m√©dias dos √≠ndices na √°rea
def calcular_media_indices_area(colecao, roi):
    def processar_imagem(img):
        data_img = img.date().format('YYYY-MM-dd')
        medias = img.select(bandas).reduceRegion(
            reducer=ee.Reducer.mean(),
            geometry=roi.geometry(),
            scale=10,
            maxPixels=1e8
        )
        return ee.Feature(None, medias).set('data', data_img)

    resultados_fc = ee.FeatureCollection(colecao.map(processar_imagem))
    df = geemap.ee_to_df(resultados_fc)
    df['data'] = pd.to_datetime(df['data'])
    return df.sort_values('data')

# 7. Calcular m√©dias para as duas safras
df_treino = calcular_media_indices_area(colecao_treino, roi)
df_pred = calcular_media_indices_area(colecao_pred, roi)

# 8. Configurar e plotar os gr√°ficos
plt.figure(figsize=(12, 8))
cores = {'CCCI': 'blue', 'NDMI': 'green', 'NDVI': 'red', 'GNDVI': 'purple', 'NDVI': 'green', 'NDWI': 'blue', 'NBR': 'yellow', 'TWI2': 'blue', 'NDRE': 'green', 'MSAVI2': 'green'}
estilos = {'2023/24': '--', '2024/25': '-'}

for i, indice in enumerate(bandas, 1):
    plt.subplot(3,3, i)
    plt.plot(df_treino['data'], df_treino[indice],
             label='Safra 2023/24', color=cores[indice],
             linestyle=estilos['2023/24'], marker='o')
    plt.plot(df_pred['data'], df_pred[indice],
             label='Safra 2024/25', color=cores[indice],
             linestyle=estilos['2024/25'], marker='s')

    plt.title(f'Evolu√ß√£o do {indice}')
    plt.ylabel('Valor M√©dio')
    plt.legend()
    plt.grid(True)
    plt.xticks(rotation=45)

plt.tight_layout()
plt.show()

# === IMPORTA√á√ïES ===
import pandas as pd
import geopandas as gpd
import numpy as np
import matplotlib.pyplot as plt
from scipy.interpolate import Rbf
from matplotlib.path import Path
from pyproj import Transformer
from osgeo import gdal, osr

# === 1. LER ARQUIVOS ===
# Caminho dos dados preditos da safra futura (do c√≥digo 1)
csv_predito = '/content/predicao_sem_datas.csv'
gdf_area = gpd.read_file('/content/area_poligono.gpkg').to_crs("EPSG:4326")

# L√™ o CSV com os dados previstos e extrai coordenadas da geometria
df_pred = pd.read_csv(csv_predito)
gdf_pred = gpd.read_file('/content/pontos_produtividade.gpkg').to_crs("EPSG:4326")
df_pred['geometry'] = gdf_pred['geometry']  # adiciona geometria real

# Converte para GeoDataFrame
gdf_pred = gpd.GeoDataFrame(df_pred, geometry='geometry', crs="EPSG:4326")

# Transforma para UTM (SIRGAS 2000 / UTM zone 22S)
gdf_pred = gdf_pred.to_crs(epsg=32722)
gdf_area = gdf_area.to_crs(epsg=32722)

# === 2. CRIAR GRID REGULAR PARA INTERPOLA√á√ÉO ===
xmin, ymin, xmax, ymax = gdf_area.total_bounds
res = 5  # resolu√ß√£o em metros
xi = np.arange(xmin, xmax, res)
yi = np.arange(ymin, ymax, res)
xi_grid, yi_grid = np.meshgrid(xi, yi)

# Dados para interpola√ß√£o
xyz = np.array([[p.x, p.y, val] for p, val in zip(gdf_pred.geometry, gdf_pred['produtividade_kg'])])

# === 3. INTERPOLA√á√ÉO SPLINE ===
def spline_interpolation(xyz, xi, yi):
    rbf = Rbf(xyz[:, 0], xyz[:, 1], xyz[:, 2], function='thin_plate')
    return rbf(xi, yi)

zi = spline_interpolation(xyz, xi_grid, yi_grid)

# === 4. APLICAR M√ÅSCARA DO POL√çGONO DA √ÅREA ===
def apply_mask(zi, xi_grid, yi_grid, gdf_area):
    poly_path = Path(gdf_area.geometry.unary_union.exterior.coords)
    mask = np.array([poly_path.contains_point((x, y)) for x, y in zip(xi_grid.flatten(), yi_grid.flatten())])
    zi_masked = np.full_like(zi, np.nan)
    zi_masked_flat = zi_masked.flatten()
    zi_masked_flat[mask] = zi.flatten()[mask]
    return zi_masked_flat.reshape(zi.shape)

zi_masked = apply_mask(zi, xi_grid, yi_grid, gdf_area)

# === 5. PLOTAGEM DO MAPA ===
fig, ax = plt.subplots(figsize=(10, 8))
img = ax.imshow(
    zi_masked,
    extent=(xmin, xmax, ymin, ymax),
    origin='lower',
    cmap='YlOrBr',
    interpolation='nearest'
)

gdf_area.boundary.plot(ax=ax, color='black', linewidth=1)
gdf_pred.plot(ax=ax, color='blue', markersize=20)
cbar = plt.colorbar(img, ax=ax, shrink=0.7)
cbar.set_label('Produtividade Prevista (kg)')

# Rosa dos ventos
ax.annotate('N', xy=(0.95, 0.95), xycoords='axes fraction', fontsize=12, fontweight='bold')
ax.annotate('', xy=(0.95, 0.92), xytext=(0.95, 0.95), xycoords='axes fraction',
            arrowprops=dict(arrowstyle='->', lw=1.5))

# Escala
ax.plot([0.85, 0.90], [0.05, 0.05], color='black', linewidth=3, transform=ax.transAxes)
ax.text(0.875, 0.07, '100 m', transform=ax.transAxes, ha='center', fontsize=9)

# T√≠tulo e eixos
ax.set_title('Variabilidade Espacial da Produtividade Prevista - Spline')
ax.set_xlabel("UTM Easting (m)")
ax.set_ylabel("UTM Northing (m)")
plt.tight_layout()

# Coordenadas ao passar o mouse
transformer = Transformer.from_crs("EPSG:32722", "EPSG:4326", always_xy=True)
def format_coord(x, y):
    lon, lat = transformer.transform(x, y)
    return f"X={x:.1f}, Y={y:.1f} | Lon={lon:.5f}, Lat={lat:.5f}"
ax.format_coord = format_coord

# Rodap√©
ax.text(0.5, -0.12, "SCR: WGS84 / 24S | Safra do Caf√©",
        transform=ax.transAxes, ha='center', va='center', fontsize=9)

plt.show()

# === 6. SALVAR TIFF DO MAPA ===
def save_as_geotiff(output_path, array, xmin, xmax, ymin, ymax, res, crs_epsg=32722):
    cols = array.shape[1]
    rows = array.shape[0]
    origin_x = xmin
    origin_y = ymin

    driver = gdal.GetDriverByName('GTiff')
    out_raster = driver.Create(output_path, cols, rows, 1, gdal.GDT_Float32)
    out_raster.SetGeoTransform((origin_x, res, 0, origin_y, 0, res))

    out_raster_srs = osr.SpatialReference()
    out_raster_srs.ImportFromEPSG(crs_epsg)
    out_raster.SetProjection(out_raster_srs.ExportToWkt())

    out_band = out_raster.GetRasterBand(1)
    out_band.WriteArray(array)
    out_band.SetNoDataValue(np.nan)
    out_band.FlushCache()

# Salvar GeoTIFF
output_tiff_path = '/content/produtividade_prevista_interpolada.tif'
save_as_geotiff(output_tiff_path, zi_masked, xmin, xmax, ymin, ymax, res)

print(f"GeoTIFF salvo em: {output_tiff_path}")

import matplotlib.pyplot as plt
import seaborn as sns
from scipy.spatial.distance import euclidean
from scipy.stats import zscore
import numpy as np

# Lista de √≠ndices
indices = [f"{b}_{stat}" for b in bandas for stat in ['min', 'mean', 'max']]

print("\n=== Comparing distributions between PREVIOUS SEASON and PREDICTED SEASON ===")

# Normaliza√ß√£o Z-score
treino_z = zscore(dados_treino[indices], axis=0, nan_policy='omit')
pred_z = zscore(dados_predicao[indices], axis=0, nan_policy='omit')

# M√©dias por √≠ndice
media_treino_z = np.nanmean(treino_z, axis=0)
media_pred_z = np.nanmean(pred_z, axis=0)

# Layout dos subplots (3 colunas, n√∫mero de linhas ajustado ao total de √≠ndices)
n_indices = len(indices)
ncols = 3
nrows = int(np.ceil(n_indices / ncols))

fig, axes = plt.subplots(nrows, ncols, figsize=(18, 5 * nrows))
axes = axes.flatten()

# Plotagem agrupada com subplots
for i, indice in enumerate(indices):
    distancia_individual = euclidean(
        [media_treino_z[i]],
        [media_pred_z[i]]
    )

    ax = axes[i]
    sns.kdeplot(dados_treino[indice], label='Training', fill=True, ax=ax)
    sns.kdeplot(dados_predicao[indice], label='Prediction', fill=True, ax=ax)
    ax.set_title(f"{indice}\nDist√¢ncia Euclidiana (z): {distancia_individual:.4f}")
    ax.set_xlabel("Valor")
    ax.set_ylabel("Densidade")
    ax.legend()
    ax.grid(True)

# Remove eixos extras (se houver)
for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.suptitle("Distribui√ß√µes dos √çndices: Safra Treinamento vs Safra Predi√ß√£o", fontsize=16, y=1.02)
plt.show()

# C√°lculo da dist√¢ncia global
distancia_total = euclidean(media_treino_z, media_pred_z)
print(f"\nOverall Euclidean z-score distance between seasons: {distancia_total:.4f}")

if distancia_total < 1.0:
    print(">> The spectral conditions of the new season are very similar to the previous one.")
elif distancia_total < 2.5:
    print(">> Moderate differences detected between seasons. Model still applicable, but use with caution.")
else:
    print(">> Significant spectral differences detected. Consider reviewing the input data or retraining the model.")

# Interpreta√ß√£o
def interpret_zscore_distance(distancia):
    print("\n" + "="*60)
    print("INTERPRETATION OF Z-SCORE DISTANCE BETWEEN SEASONS".center(60))
    print("="*60)
    print("\nPractical interpretation of the distance values:")
    print(f"{'Distance':<15} {'Interpretation'}")
    print(f"{'~ 0.0':<15} Spectrally almost identical seasons")
    print(f"{'0.5 ‚Äì 1.5':<15} Small differences ‚Äî model likely still valid")
    print(f"{'1.5 ‚Äì 2.5':<15} Moderate differences ‚Äî assess model sensitivity")
    print(f"{'> 2.5':<15} Significant differences ‚Äî consider new calibration")

    print("\nAutomatic summary based on calculated distance:")
    if distancia < 0.5:
        print(">> Spectrally almost identical seasons.")
    elif distancia < 1.5:
        print(">> Small differences ‚Äî model likely still valid.")
    elif distancia < 2.5:
        print(">> Moderate differences ‚Äî assess model sensitivity.")
    else:
        print(">> Significant differences ‚Äî new calibration or model may be needed.")

# Executa interpreta√ß√£o
interpret_zscore_distance(distancia_total)

# GERAR PONTOS √ÅREA TOTAL

import geopandas as gpd
import numpy as np
from shapely.geometry import Point
import math
import pandas as pd
import joblib
from sklearn.ensemble import GradientBoostingRegressor
import warnings
warnings.filterwarnings('ignore')

### GERAR PONTOS NA √ÅREA TOTAL

# 1. Carregar pol√≠gono e calcular √°rea (ha)
area_total = gpd.read_file('/content/area_total_poligono.gpkg')

def calcular_area_ha(gdf):
    if gdf.crs.is_geographic:
        utm_zone = int((gdf.total_bounds[0] + 180) // 6 + 1)
        crs_utm = f'EPSG:326{utm_zone:02d}' if gdf.total_bounds[1] >= 0 else f'EPSG:327{utm_zone:02d}'
        gdf = gdf.to_crs(crs_utm)
    return gdf.geometry.area.sum() / 10000, gdf

area_ha_total, area_total_proj = calcular_area_ha(area_total)
num_pontos_total = int(round(2 * area_ha_total))
print(f"√Årea total: {area_ha_total:.2f} ha | Pontos a gerar: {num_pontos_total}")

# 2. Gerar pontos regulares e complementares
def gerar_pontos(polygon_gdf, num_points):
    minx, miny, maxx, maxy = polygon_gdf.total_bounds
    spacing = math.sqrt(polygon_gdf.geometry.area.sum() / num_points)
    x_coords = np.linspace(minx, maxx, int((maxx - minx) / spacing) + 2)
    y_coords = np.linspace(miny, maxy, int((maxy - miny) / spacing) + 2)

    points = []
    for x in x_coords:
        for y in y_coords:
            p = Point(x, y)
            if polygon_gdf.geometry.contains(p).any():
                points.append(p)
            if len(points) >= num_points:
                break
        if len(points) >= num_points:
            break

    while len(points) < num_points:
        p = Point(np.random.uniform(minx, maxx), np.random.uniform(miny, maxy))
        if polygon_gdf.geometry.contains(p).any() and p not in points:
            points.append(p)

    return points[:num_points]

pontos = gerar_pontos(area_total_proj, num_pontos_total)
gdf_pontos = gpd.GeoDataFrame(geometry=pontos, crs=area_total_proj.crs)
gdf_pontos.to_file('/content/pontos_area_total.gpkg', driver='GPKG')
print("Arquivo de pontos salvo com sucesso!")

## PROCESSAR DADOS

import ee
import geemap
import pandas as pd
from datetime import datetime

# exportar os arquivos GPKG
if gdf_poligono is not None:
    # Exportar pol√≠gono
    poligono_path = '/content/area_total_poligono.gpkg'
    gdf_poligono.to_file(poligono_path, driver='GPKG')
    print(f"\n‚úÖ Arquivo do pol√≠gono exportado: {poligono_path}")

    # Verificar e exportar pontos se existirem
    if gdf_pontos is not None and not gdf_pontos.empty:
        pontos_path = '/content/pontos_area_total.gpkg'
        gdf_pontos.to_file(pontos_path, driver='GPKG')
        print(f"‚úÖ Arquivo de pontos exportado: {pontos_path}")
        print(f"Total de pontos exportados: {len(gdf_pontos)}")
    else:
        print("\n‚ÑπÔ∏è Nenhum ponto de produtividade foi adicionado para exporta√ß√£o")
else:
    print("\n‚ö†Ô∏è Nenhuma √°rea definida para exporta√ß√£o")

arquivo_poligono = '/content/area_total_poligono.gpkg'
arquivo_pontos = '/content/pontos_area_total.gpkg'

# Leitura dos arquivos
gdf_poligono = gpd.read_file(arquivo_poligono)
gdf_pontos = gpd.read_file(arquivo_pontos)

# Garantir que o CRS esteja em metros
if gdf_poligono.crs.is_geographic:
    gdf_poligono = gdf_poligono.to_crs(epsg=32723)

# C√°lculo da √°rea total em hectares
area_ha = gdf_poligono.geometry.area.sum() / 10000
print(f"\n√Årea total calculada a partir do pol√≠gono carregado: {area_ha:.2f} hectares")

# Mostrar conte√∫do do GeoDataFrame de pontos
print("\nDados do GeoDataFrame de pontos:")
display(gdf_pontos.head())

# Mostrar a contagem de fei√ß√µes e os nomes das colunas
print(f"\nPol√≠gono cont√©m {len(gdf_poligono)} fei√ß√µes e as colunas: {list(gdf_poligono.columns)}")
print(f"Pontos cont√™m {len(gdf_pontos)} fei√ß√µes e as colunas: {list(gdf_pontos.columns)}")

# Converter para objetos do Earth Engine
poligono = geemap.gdf_to_ee(gdf_poligono)
pontos = geemap.gdf_to_ee(gdf_pontos)

# Intervalo de datas da sele√ß√£o de imagens do sat√©lite Sentinel-2
data_inicio = '2024-08-01'
data_fim = '2025-05-31'

data_inicio_ee = ee.Date(data_inicio)
data_fim_ee = ee.Date(data_fim)

# Calcular √≠ndices espectrais
def calcular_indices(image):
    ndvi = image.normalizedDifference(['B8', 'B4']).rename('NDVI')
    ndre = image.normalizedDifference(['B8', 'B5']).rename('NDRE')
    ccci = ndre.divide(ndvi).rename('CCCI')
    gndvi = image.normalizedDifference(['B8', 'B3']).rename('GNDVI')
    ndmi = image.normalizedDifference(['B8', 'B11']).rename('NDMI')
    msavi2 = image.expression(
        '(2 * NIR + 1 - sqrt((2 * NIR + 1) ** 2 - 8 * (NIR - RED))) / 2', {
            'NIR': image.select('B8'),
            'RED': image.select('B4')
        }).rename('MSAVI2')
    nbr = image.normalizedDifference(['B8', 'B12']).rename('NBR')
    twi2 = image.normalizedDifference(['B9', 'B8']).rename('TWI2')
    ndwi = image.normalizedDifference(['B3', 'B8']).rename('NDWI')
    return image.addBands([ndvi, ndre, ccci, gndvi, ndmi, msavi2, nbr, twi2, ndwi])

# Filtrar cole√ß√£o Sentinel-2
colecao = ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED') \
    .filterBounds(poligono.geometry()) \
    .filterDate(data_inicio_ee, data_fim_ee) \
    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20)) \
    .map(calcular_indices)

# Validar imagens com dados
def imagem_valida(img):
    count = img.select(0).reduceRegion(
        reducer=ee.Reducer.count(),
        geometry=poligono,
        scale=10,
        maxPixels=1e9
    ).values().get(0)
    return img.set('valida', ee.Number(count).gt(0))

colecao_com_valida = colecao.map(imagem_valida)
imagens_validas = colecao_com_valida.filter(ee.Filter.eq('valida', 1))

def add_formatted_date(img):
    date_str = ee.Date(img.get('system:time_start')).format('YYYY-MM-dd')
    return img.set('formatted_date', date_str)

imagens_validas = imagens_validas.map(add_formatted_date)
distinct_dates = imagens_validas.distinct(['formatted_date'])
imagens_validas = ee.ImageCollection(distinct_dates)

datas = imagens_validas.aggregate_array('formatted_date').getInfo()

if datas:
    print(f"{len(datas)} imagens v√°lidas:")
    for data in sorted(datas):
        print(f"- {data}")
else:
    print("\nNenhuma imagem v√°lida encontrada com dados na √°rea especificada.")

# Definir os √≠ndices que ser√£o extra√≠dos
nomes_indices = ['NDVI', 'NDRE', 'CCCI', 'GNDVI', 'NDMI', 'MSAVI2', 'NBR', 'TWI2', 'NDWI']

# Fun√ß√£o para extrair estat√≠sticas por ponto
def extrair_estatisticas_ponto_imagem(imagem, feature_ponto, nomes_indices):
    buffer = feature_ponto.geometry().buffer(5)  # Buffer de 5 metros
    data_img = ee.Date(imagem.get('system:time_start')).format('yyyyMMdd')
    resultado_img = ee.Dictionary({})

    for indice in nomes_indices:
        banda = imagem.select(indice)
        reducao = banda.reduceRegion(
            reducer=ee.Reducer.min().combine(
                reducer2=ee.Reducer.mean(), sharedInputs=True
            ).combine(
                reducer2=ee.Reducer.max(), sharedInputs=True
            ),
            geometry=buffer,
            scale=10,
            maxPixels=1e8
        )

        valor_min = reducao.get(indice + '_min')
        valor_mean = reducao.get(indice + '_mean')
        valor_max = reducao.get(indice + '_max')

        chave_min = ee.String(data_img).cat('_').cat(indice).cat('_min')
        chave_mean = ee.String(data_img).cat('_').cat(indice).cat('_mean')
        chave_max = ee.String(data_img).cat('_').cat(indice).cat('_max')

        resultado_img = ee.Dictionary(ee.Algorithms.If(valor_min, resultado_img.set(chave_min, valor_min), resultado_img))
        resultado_img = ee.Dictionary(ee.Algorithms.If(valor_mean, resultado_img.set(chave_mean, valor_mean), resultado_img))
        resultado_img = ee.Dictionary(ee.Algorithms.If(valor_max, resultado_img.set(chave_max, valor_max), resultado_img))

    return ee.Feature(feature_ponto.geometry(), resultado_img)

# Processar todos os pontos
def processar_ponto(ponto, colecao_imagens, lista_indices):
    def extrair_para_cada_imagem(imagem):
        return extrair_estatisticas_ponto_imagem(imagem, ponto, lista_indices)

    resultados_por_imagem = colecao_imagens.map(extrair_para_cada_imagem)

    def combinar_props(feature, acumulador):
        return ee.Dictionary(acumulador).combine(ee.Feature(feature).toDictionary(), overwrite=True)

    propriedades_combinadas = ee.Dictionary(
        resultados_por_imagem.iterate(combinar_props, ee.Dictionary({}))
    )

    return ee.Feature(ponto.geometry(), propriedades_combinadas)

# Executar processamento
pontos_com_indices = pontos.map(lambda pt: processar_ponto(pt, imagens_validas, nomes_indices))

# Converter para GeoDataFrame em mem√≥ria
gdf_resultado = geemap.ee_to_gdf(pontos_com_indices)

# Separar vers√£o sem geometria (em mem√≥ria, sem salvar)
df_sem_geometria = gdf_resultado.drop(columns=['geometry'])

csv_limpo = '/content/indices_24_25.csv'
colunas_remover = ['geometry']
gdf_resultado.drop(columns=[col for col in colunas_remover if col in gdf_resultado.columns]) \
              .to_csv(csv_limpo, index=False)
print(f"Arquivo salvo sem geometria: {csv_limpo}")

# Mostrar os dados limpos
print("Dados do arquivo:")
print(pd.read_csv(csv_limpo))

import geopandas as gpd
import numpy as np
from shapely.geometry import Point
import math
import pandas as pd
import joblib
from sklearn.ensemble import GradientBoostingRegressor
import warnings
warnings.filterwarnings('ignore')


# 3. Fun√ß√£o para tratar colunas
def remover_datas_colunas(df):
    df = df.copy()
    df.columns = [col.split('_', 1)[-1] if '_' in col else col for col in df.columns]
    return df.loc[:, ~df.columns.duplicated()]

# 4. Carregar dados de treino
df_treino = pd.read_csv('/content/indices_23_24_limpo.csv')
df_treino = remover_datas_colunas(df_treino)

if 'kg' not in df_treino.columns:
    raise KeyError("Coluna 'kg' n√£o encontrada nos dados de treino!")

X = df_treino.drop(columns=['kg'])
y = df_treino['kg']
print(f"Colunas de treino: {len(X.columns)}")

# 5. Treinar modelo
modelo = GradientBoostingRegressor()
modelo.fit(X, y)

# 6. Carregar dados de predi√ß√£o e validar colunas
df_pred = pd.read_csv('/content/indices_24_25.csv')
df_pred = remover_datas_colunas(df_pred)

colunas_modelo = X.columns.tolist()
colunas_faltando = [col for col in colunas_modelo if col not in df_pred.columns]
if colunas_faltando:
    raise ValueError(f"Colunas faltando nos dados de predi√ß√£o: {colunas_faltando}")

X_pred = df_pred[colunas_modelo]

# 7. Fazer predi√ß√£o
y_pred = modelo.predict(X_pred)
df_pred['produtividade_kg'] = y_pred
df_pred['produtividade_sc/ha'] = y_pred * (1 / 60) * (1 / 0.0016)

media_predita = df_pred['produtividade_sc/ha'].mean()
print(f"Produtividade m√©dia prevista: {media_predita:.2f} sc/ha")

# 8. Salvar resultados
df_pred.to_csv('/content/predicao_safra_2025.csv', index=False)
print("Predi√ß√£o salva em: /content/predicao_safra_2025.csv")
